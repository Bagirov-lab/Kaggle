{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6950c9d",
   "metadata": {
    "papermill": {
     "duration": 0.00442,
     "end_time": "2023-08-01T13:19:22.317343",
     "exception": false,
     "start_time": "2023-08-01T13:19:22.312923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Idea\n",
    "\n",
    "Research applicability of TabPFN model.\n",
    "\n",
    "TODO:\n",
    "- [ ] Shall add Cut Offs\n",
    "- [ ] Shall One model version based on CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693d6f",
   "metadata": {
    "papermill": {
     "duration": 0.003607,
     "end_time": "2023-08-01T13:19:22.325045",
     "exception": false,
     "start_time": "2023-08-01T13:19:22.321438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install TabPFN offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef4f139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T13:19:22.334932Z",
     "iopub.status.busy": "2023-08-01T13:19:22.334419Z",
     "iopub.status.idle": "2023-08-01T13:19:38.470760Z",
     "shell.execute_reply": "2023-08-01T13:19:38.469295Z"
    },
    "papermill": {
     "duration": 16.14496,
     "end_time": "2023-08-01T13:19:38.473869",
     "exception": false,
     "start_time": "2023-08-01T13:19:22.328909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/pip-packages-icr\r\n",
      "Processing /kaggle/input/pip-packages-icr/tabpfn-0.1.9-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (1.23.5)\r\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (5.4.1)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (2.28.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (1.2.2)\r\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (2.0.0+cpu)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (2023.5.7)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.10.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (3.1.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.12.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->tabpfn) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->tabpfn) (1.3.0)\r\n",
      "Installing collected packages: tabpfn\r\n",
      "Successfully installed tabpfn-0.1.9\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr\n",
    "\n",
    "!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "\n",
    "!cp /kaggle/input/pip-packages-icr/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a832ecc",
   "metadata": {
    "papermill": {
     "duration": 0.004029,
     "end_time": "2023-08-01T13:19:38.482608",
     "exception": false,
     "start_time": "2023-08-01T13:19:38.478579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3797ebd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T13:19:38.493380Z",
     "iopub.status.busy": "2023-08-01T13:19:38.492939Z",
     "iopub.status.idle": "2023-08-01T13:19:40.453989Z",
     "shell.execute_reply": "2023-08-01T13:19:40.452605Z"
    },
    "papermill": {
     "duration": 1.969619,
     "end_time": "2023-08-01T13:19:40.456683",
     "exception": false,
     "start_time": "2023-08-01T13:19:38.487064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from woe_utils import WOENumericalComplex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd152a4",
   "metadata": {
    "papermill": {
     "duration": 0.004211,
     "end_time": "2023-08-01T13:19:40.465618",
     "exception": false,
     "start_time": "2023-08-01T13:19:40.461407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Standard Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb1eed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T13:19:40.476959Z",
     "iopub.status.busy": "2023-08-01T13:19:40.476522Z",
     "iopub.status.idle": "2023-08-01T13:19:54.661464Z",
     "shell.execute_reply": "2023-08-01T13:19:54.659721Z"
    },
    "papermill": {
     "duration": 14.194194,
     "end_time": "2023-08-01T13:19:54.664431",
     "exception": false,
     "start_time": "2023-08-01T13:19:40.470237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "from keras import metrics # accuracy\n",
    "from keras import backend as K\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator \n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee48d4",
   "metadata": {
    "papermill": {
     "duration": 0.004136,
     "end_time": "2023-08-01T13:19:54.674753",
     "exception": false,
     "start_time": "2023-08-01T13:19:54.670617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d511153c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T13:19:54.685829Z",
     "iopub.status.busy": "2023-08-01T13:19:54.684981Z",
     "iopub.status.idle": "2023-08-01T13:19:54.718600Z",
     "shell.execute_reply": "2023-08-01T13:19:54.717366Z"
    },
    "papermill": {
     "duration": 0.042294,
     "end_time": "2023-08-01T13:19:54.721456",
     "exception": false,
     "start_time": "2023-08-01T13:19:54.679162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = pd.read_pickle('/kaggle/input/invitro-train-feature-engineer/features.pickle')\n",
    "test = pd.read_pickle('/kaggle/input/invitro-train-feature-engineer/test_processed.pickle')\n",
    "train = pd.read_pickle('/kaggle/input/invitro-train-feature-engineer/train_processed.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2732dd3",
   "metadata": {
    "papermill": {
     "duration": 0.004365,
     "end_time": "2023-08-01T13:19:54.730930",
     "exception": false,
     "start_time": "2023-08-01T13:19:54.726565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model\n",
    "\n",
    "Today, we will use the defaults to create the Random Forest Model. By default the model is set to train for a classification task.\n",
    "We will train a model for each fold and after training we will store the model and metrics. Here, we have chosen `accuracy` and `binary_crossentropy` as the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a84b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T13:19:54.742967Z",
     "iopub.status.busy": "2023-08-01T13:19:54.742529Z",
     "iopub.status.idle": "2023-08-01T13:19:54.777849Z",
     "shell.execute_reply": "2023-08-01T13:19:54.775948Z"
    },
    "papermill": {
     "duration": 0.044902,
     "end_time": "2023-08-01T13:19:54.780684",
     "exception": false,
     "start_time": "2023-08-01T13:19:54.735782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balanced_logloss_np(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \n",
    "    # Correct Values\n",
    "    min_val = 1e-15\n",
    "    max_val = 0.999999999999999\n",
    "\n",
    "    y_pred = np.minimum(y_pred, [max_val])\n",
    "    y_pred = np.maximum(y_pred, [min_val])\n",
    "    \n",
    "    y_pred_1 = y_pred\n",
    "    y_pred_0 = 1-y_pred\n",
    "\n",
    "    log_y_pred_1 = np.reshape(np.log(y_pred_1),[-1,1])\n",
    "    log_y_pred_0 = np.reshape(np.log(y_pred_0),[-1,1])\n",
    "\n",
    "    y_1 = np.reshape(y_true,[1,-1])\n",
    "    y_0 = (y_1-1)*(-1)\n",
    "\n",
    "    logloss_1 = -np.dot(y_1,log_y_pred_1)[0][0]/np.sum(y_1)\n",
    "    logloss_0 = -np.dot(y_0,log_y_pred_0)[0][0]/np.sum(y_0)\n",
    "\n",
    "    av_logloss = (logloss_1+logloss_0)/2\n",
    "    \n",
    "    return av_logloss\n",
    "\n",
    "def train_model_tabpfn_one(train: pd.DataFrame, test: pd.DataFrame, features: list, label = \"Class\") -> (pd.DataFrame, dict, dict):\n",
    "\n",
    "    # Create a dataframe of required size with zero values.\n",
    "    test_summary = pd.DataFrame(data=np.full((len(test.index),1), np.nan),index=test.index)\n",
    "    train_summary = pd.DataFrame(data=np.full((len(train.index),1), np.nan),index=train.index)\n",
    "    # Create an empty dictionary to store the models trained for each fold.\n",
    "    metrics = {}\n",
    "\n",
    "    # Select only feature columns for training.\n",
    "    train_df = train[features+[label]]\n",
    "\n",
    "    # Define & Train the model and metrics\n",
    "    model = TabPFNClassifier(N_ensemble_configurations=64)\n",
    "    model.fit(train_df[features],train_df[label])\n",
    "\n",
    "    # Make predictions\n",
    "    p_train = model.predict_proba(train_df[features])[:,1]\n",
    "    p_test = model.predict_proba(test[features])[:,1]\n",
    "\n",
    "    # Predict value for validation/Submition data\n",
    "    test_summary[0] = p_test.flatten() \n",
    "    train_summary[0] = p_train.flatten() \n",
    "    \n",
    "    # Evaluate and store the metrics in respective dicts\n",
    "    metrics['balanced_logloss'] = balanced_logloss_np(y_true=train_df[label].values,y_pred=p_train)\n",
    "    print(f\"\\nTrain: {metrics['balanced_logloss']:.4f}\")\n",
    "    \n",
    "            \n",
    "    return train_summary, test_summary, model, metrics\n",
    "\n",
    "class TabPFN_CV_Ensemble(RegressorMixin,BaseEstimator):\n",
    "    def __init__(self, model_obj = TabPFNClassifier, label = \"Class\"):\n",
    "        self.label: str = label\n",
    "        self.model_obj = model_obj\n",
    "        \n",
    "        # Empty\n",
    "        self.X_summary: pd.DataFrame = pd.DataFrame()\n",
    "        self.valid_summary: pd.DataFrame = pd.DataFrame()\n",
    "        self.features: list = list()\n",
    "        self.models: dict = dict()\n",
    "        self.metrics: dict = dict()\n",
    "        \n",
    "        \n",
    "    def _compute_weights(self, df: pd.DataFrame) -> dict:\n",
    "        # Calculate the number of samples for each label.\n",
    "        neg, pos = np.bincount(df[self.label])\n",
    "        total = neg + pos\n",
    "        weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "        weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        \n",
    "        return class_weight\n",
    "        \n",
    "    def fit(self, X: pd.DataFrame, features: list, splitter = StratifiedKFold(),\n",
    "            model_kwargs = dict(N_ensemble_configurations=64)):\n",
    "        \n",
    "        # Case CV Ensmble\n",
    "        # TODO: Case of N_ensemble_configurations\n",
    "        \n",
    "        n_splits = splitter.get_n_splits()\n",
    "\n",
    "        # Create a various frames\n",
    "        self.X_summary = pd.DataFrame(data=np.full((len(X.index),n_splits), np.nan), index=X.index) # For In-Sample Predictions of each Fold\n",
    "        self.valid_summary = pd.DataFrame(data=np.full((len(X.index),1), np.nan), index=X.index) # For Out-of-Sample Prediction of each Fold\n",
    "        self.features: list = features\n",
    "        \n",
    "        # Create an empty dictionary to store the models Xed for each fold.\n",
    "        self.models = {}\n",
    "        self.metrics = {}\n",
    "        balanced_logloss_train = {}\n",
    "        balanced_logloss_val = {}\n",
    "\n",
    "        class_weight: dict = self._compute_weights(X)\n",
    "        \n",
    "        for i, (train_index, valid_index) in enumerate(splitter.split(X=X,y=X[self.label])):\n",
    "            print('##### Fold',i)\n",
    "            # Fetch values corresponding to the index \n",
    "            train_df = X.iloc[train_index]\n",
    "            valid_df = X.iloc[valid_index]\n",
    "            train_ids = train_df.index.values\n",
    "            valid_ids = valid_df.index.values\n",
    "\n",
    "            # Select only feature columns for training.\n",
    "            train_df = train_df[self.features+[self.label]]\n",
    "            valid_df = valid_df[self.features+[self.label]]\n",
    "\n",
    "            # Define & Train the model\n",
    "            model = self.model_obj(**model_kwargs)\n",
    "            model.fit(train_df[self.features],train_df[self.label])\n",
    "\n",
    "            # Store the model\n",
    "            self.models[i] = model\n",
    "\n",
    "            # Predict value for validation/Submition data\n",
    "            p_train = model.predict_proba(train_df[self.features])[:,1]\n",
    "            p_val = model.predict_proba(valid_df[self.features])[:,1]\n",
    "\n",
    "            # Predict Values\n",
    "            self.X_summary.loc[train_ids, i] = p_train\n",
    "            self.valid_summary.loc[valid_ids, 0] = p_val\n",
    "\n",
    "            # Evaluate and store the metrics in respective dicts\n",
    "            train_metric = balanced_logloss_np(y_true=train_df[self.label].values,y_pred=p_train)\n",
    "            val_metric = balanced_logloss_np(y_true=valid_df[self.label].values,y_pred=p_val)\n",
    "\n",
    "            balanced_logloss_train[i] = train_metric\n",
    "            balanced_logloss_val[i] = val_metric\n",
    "\n",
    "            print(f\"\\nTrain: {train_metric:.4f} Validation: {val_metric:.4f}\")\n",
    "    \n",
    "        self.metrics['train'] = balanced_logloss_train\n",
    "        self.metrics['val'] = balanced_logloss_val\n",
    "\n",
    "        print(f\"\\nTrain mean: {pd.Series(balanced_logloss_train).mean():.4f} std: {pd.Series(balanced_logloss_train).std():.4f}\")\n",
    "        print(f\"\\nValidation mean: {pd.Series(balanced_logloss_val).mean():.4f} std: {pd.Series(balanced_logloss_val).std():.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        n_splits = len(self.models)\n",
    "        X_summary = pd.DataFrame(data=np.full((len(X.index),n_splits), np.nan),index=X.index) # For X (Sumbition) Predictions of each Fold's Model\n",
    "\n",
    "        for i, model in enumerate(self.models.values()):\n",
    "            X_summary[i] = model.predict_proba(X[self.features])[:,1]\n",
    "        \n",
    "        return X_summary\n",
    "    \n",
    "    def save(self, save_path: str) -> None:\n",
    "        try:\n",
    "            shutil.rmtree(save_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        os.makedirs(f'{save_path}/models', exist_ok=True)\n",
    "        \n",
    "        for fold, model in self.models.items():\n",
    "            joblib.dump(value=model, filename=f'{save_path}/models/{fold}.pickle')\n",
    "        \n",
    "        joblib.dump(value=self.label, filename=f'{save_path}/label.pickle')\n",
    "        joblib.dump(value=self.model_obj, filename=f'{save_path}/model_obj.pickle')\n",
    "        \n",
    "        joblib.dump(value=self.X_summary, filename=f'{save_path}/X_summary.pickle')\n",
    "        joblib.dump(value=self.valid_summary, filename=f'{save_path}/valid_summary.pickle')\n",
    "        joblib.dump(value=self.features, filename=f'{save_path}/features.pickle')\n",
    "        joblib.dump(value=self.metrics, filename=f'{save_path}/metrics.pickle')\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def load(self, save_path: str):\n",
    "        \n",
    "        self.label = joblib.load(filename=f'{save_path}/label.pickle')\n",
    "        self.model_obj = joblib.load(filename=f'{save_path}/model_obj.pickle')\n",
    "        \n",
    "        self.X_summary = joblib.load(filename=f'{save_path}/X_summary.pickle')\n",
    "        self.valid_summary = joblib.load(filename=f'{save_path}/valid_summary.pickle')\n",
    "        self.features = joblib.load(filename=f'{save_path}/features.pickle')\n",
    "        self.metrics = joblib.load(filename=f'{save_path}/metrics.pickle')\n",
    "        \n",
    "        self.models = dict()\n",
    "        \n",
    "        for name in os.listdir(f'{save_path}/models'):\n",
    "            i = name.split('.')[0]\n",
    "            self.models[int(i)] = joblib.load(filename=f'{save_path}/models/{name}')\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f59641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T06:36:40.208129Z",
     "iopub.status.busy": "2023-07-22T06:36:40.207653Z",
     "iopub.status.idle": "2023-07-22T06:49:24.120496Z",
     "shell.execute_reply": "2023-07-22T06:49:24.11916Z",
     "shell.execute_reply.started": "2023-07-22T06:36:40.208021Z"
    },
    "papermill": {
     "duration": 0.005898,
     "end_time": "2023-08-01T13:19:54.791083",
     "exception": false,
     "start_time": "2023-08-01T13:19:54.785185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before\n",
    "```python\n",
    "# TabPFN tabpfn_\n",
    "features_slice = [i for i in features if len(i) > 2]\n",
    "train_summary_tabpfn_2, valid_summary_tabpfn_2, test_summary_tabpfn_2, model_tabpfn_2, metrics_tabpfn_2 = train_model_tabpfn_cv(\n",
    "    train=train_out,test=test_out, features=features_slice, n_splits=6)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f561745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T13:19:54.803228Z",
     "iopub.status.busy": "2023-08-01T13:19:54.802783Z",
     "iopub.status.idle": "2023-08-01T13:21:37.847858Z",
     "shell.execute_reply": "2023-08-01T13:21:37.846695Z"
    },
    "papermill": {
     "duration": 103.057259,
     "end_time": "2023-08-01T13:21:37.854021",
     "exception": false,
     "start_time": "2023-08-01T13:19:54.796762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "\n",
      "Train: 0.0515\n",
      "CPU times: user 2min 32s, sys: 48.6 s, total: 3min 20s\n",
      "Wall time: 1min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/TabPFN/1/1.pickle']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# TabPFN tabpfn_\n",
    "\n",
    "features_slice = [i for i in features if len(i) > 2]\n",
    "train_summary_tabpfn_1, test_summary_tabpfn_1, model_tabpfn_1, metrics_tabpfn_1 = train_model_tabpfn_one(\n",
    "    train=train,test=test, features=features_slice)\n",
    "\n",
    "os.makedirs('/kaggle/working/TabPFN/1',exist_ok=True)\n",
    "joblib.dump(value=model_tabpfn_1, filename='/kaggle/working/TabPFN/1/1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c639a596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T13:21:37.866778Z",
     "iopub.status.busy": "2023-08-01T13:21:37.866334Z",
     "iopub.status.idle": "2023-08-01T13:33:05.319698Z",
     "shell.execute_reply": "2023-08-01T13:33:05.318385Z"
    },
    "papermill": {
     "duration": 687.467909,
     "end_time": "2023-08-01T13:33:05.327130",
     "exception": false,
     "start_time": "2023-08-01T13:21:37.859221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Fold 0\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "\n",
      "Train: 0.0591 Validation: 0.1325\n",
      "##### Fold 1\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "\n",
      "Train: 0.0528 Validation: 0.3273\n",
      "##### Fold 2\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "\n",
      "Train: 0.0505 Validation: 0.3091\n",
      "##### Fold 3\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "\n",
      "Train: 0.0511 Validation: 0.2504\n",
      "##### Fold 4\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "\n",
      "Train: 0.0512 Validation: 0.2377\n",
      "##### Fold 5\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "\n",
      "Train: 0.0519 Validation: 0.4155\n",
      "\n",
      "Train mean: 0.0528 std: 0.0032\n",
      "\n",
      "Validation mean: 0.2788 std: 0.0958\n",
      "CPU times: user 17min 18s, sys: 5min 9s, total: 22min 27s\n",
      "Wall time: 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TabPFN\n",
    "features_slice = [i for i in features if len(i) > 2]\n",
    "\n",
    "# Test\n",
    "my_splitter = StratifiedKFold(n_splits=6,shuffle=True, random_state=1902)\n",
    "\n",
    "# initialise\n",
    "TabPFN_CV_Ensemble_2 = TabPFN_CV_Ensemble(model_obj = TabPFNClassifier, label = \"Class\")\n",
    "\n",
    "# train\n",
    "TabPFN_CV_Ensemble_2 = TabPFN_CV_Ensemble_2.fit(X=train, features=features_slice, \n",
    "                                  splitter=my_splitter,\n",
    "                                  model_kwargs = dict(N_ensemble_configurations=64))\n",
    "\n",
    "# save\n",
    "TabPFN_CV_Ensemble_2.save(save_path='/kaggle/working/TabPFN/2')\n",
    "\n",
    "# Collect attributes and predictions\n",
    "\n",
    "train_summary_tabpfn_2 = TabPFN_CV_Ensemble_2.X_summary\n",
    "valid_summary_tabpfn_2 = TabPFN_CV_Ensemble_2.valid_summary\n",
    "test_summary_tabpfn_2 = TabPFN_CV_Ensemble_2.predict(X=test)\n",
    "model_tabpfn_2 = TabPFN_CV_Ensemble_2.models\n",
    "metrics_tabpfn_2 = TabPFN_CV_Ensemble_2.metrics\n",
    "\n",
    "# for inference load\n",
    "\n",
    "# TabPFN_CV_Ensemble_2_load = TabPFN_CV_Ensemble(model_obj = TabPFNClassifier, label = \"Class\")\n",
    "\n",
    "# TabPFN_CV_Ensemble_2_load = TabPFN_CV_Ensemble_2_load.load(save_path='/kaggle/working/TabPFN/2')\n",
    "\n",
    "\n",
    "# train_summary_tabpfn_2, valid_summary_tabpfn_2, test_summary_tabpfn_2, model_tabpfn_2, metrics_tabpfn_2 = train_model_tabpfn_cv(\n",
    "#     train=train_out,test=test_out, features=features_slice, n_splits=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 838.635357,
   "end_time": "2023-08-01T13:33:08.461813",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-01T13:19:09.826456",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
