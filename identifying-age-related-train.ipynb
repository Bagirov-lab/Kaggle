{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read Before","metadata":{}},{"cell_type":"markdown","source":"- https://www.kaggle.com/code/raddar/icr-competition-analysis-and-findings/notebook\n- https://www.tensorflow.org/guide/core/logistic_regression_core\n- https://www.kaggle.com/code/muelsamu/simple-tabpfn-approach-for-score-of-15-in-1-min/notebook - good example how to add TabPFN\n- https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric - good guide on creating custom metric in Keras\n- https://www.kaggle.com/competitions/icr-identify-age-related-conditions/discussion/410139 - overall notes about similar competitions","metadata":{}},{"cell_type":"markdown","source":"Plan:\n- [x] Feature Engineering (1 day)\n- [x] CV and Model Selection (1 day)\n- [x] Validation (1 day)\n- [x] Review\n- [x] Make Artefacts -> Made Utility script for WoE\n- [x] Solve Error With Solution -> Made If new catgory then choose worst WoEs (Can make two splits woth worst and other value )\n- [x] Add TabPFN (Added Private Sample with package files) and No CV TabPFN preds\n- [x] Added Weighted Submition with respect to Competion Metric Mean\n- [x] Refactor More Accurately train RF part & Make Dev branch for Git\n- [x] Experiment with GadientClassifier and Parameters ( Tried GadientClassifier / Added Better displayed TOC )\n- [x] CV TabPFN - looks even inferior to GBT. May be due to preprosessing.\n- [x] Ensembled into Submition results from several Classifiers (There is aa room for Playing with manually assigned weights)\n- [x] Due to time consideration -> SUBMITION: bool\n- [x] Submition Error When calculating results.\n- [x] Rethink CV (check dimension) v31->v32,\n- [x] Solve problem with accuracy (showns NaNs) -> Changed to Binary Accuracy version\n- [x] Solved problem with NaNs in metric -> min/max vals less than 1 more than 0, as in competrion calculator\n- [x] loss: 0.0000e+00. Why ? Is it OK ? -> Yes it is okey. https://discuss.tensorflow.org/t/tfdf-custom-loss/2223\n- [ ] Research wheather it is possible to train with respect to Gradient of the metric.\n- [ ] Found that some variables are constant as they represent some features for categroical column which takes only two values, so makes sense to drop them in order not overtrain. I will make split -> make two versions of model which is run if Group A (on all data), which is Group B ( which finetuned after )\n- [ ] Ensemble Results by some model:\n    - Currently Simple Manual Weights, which may be more optimal. Need to make some grid and choose the best.\n    - I use Mean for Each CV Branch, may-be also grant some threshs or weights\n    - [x] Refactored each Model Estimation to result into Train/Valid/Test Frames\n- [ ] Cutoffs\n    - https://medium.com/swlh/determining-a-cut-off-or-threshold-when-working-with-a-binary-dependent-target-variable-7c2342cf2a7c\n    - Find Lower/Upper Bounds which maximises competition metric\n- [ ] May be try several CVs\n- [ ] May be play with uplift to lessen features input\n    ","metadata":{}},{"cell_type":"code","source":"SUBMITION = False","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:15:30.213351Z","iopub.execute_input":"2023-07-29T06:15:30.213860Z","iopub.status.idle":"2023-07-29T06:15:30.250544Z","shell.execute_reply.started":"2023-07-29T06:15:30.213805Z","shell.execute_reply":"2023-07-29T06:15:30.249559Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Install TabPFN offline","metadata":{}},{"cell_type":"code","source":"!pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr\n\n!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n\n!cp /kaggle/input/pip-packages-icr/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:15:31.016182Z","iopub.execute_input":"2023-07-29T06:15:31.016626Z","iopub.status.idle":"2023-07-29T06:15:49.057114Z","shell.execute_reply.started":"2023-07-29T06:15:31.016577Z","shell.execute_reply":"2023-07-29T06:15:49.055469Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/pip-packages-icr\nProcessing /kaggle/input/pip-packages-icr/tabpfn-0.1.9-py3-none-any.whl\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (1.23.5)\nRequirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (5.4.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (2.28.2)\nRequirement already satisfied: scikit-learn>=0.24.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (1.2.2)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (2.0.0+cpu)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (2023.5.7)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.10.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->tabpfn) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->tabpfn) (1.3.0)\nInstalling collected packages: tabpfn\nSuccessfully installed tabpfn-0.1.9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Utils","metadata":{}},{"cell_type":"code","source":"from woe_utils import WOENumericalComplex","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:15:49.059731Z","iopub.execute_input":"2023-07-29T06:15:49.060142Z","iopub.status.idle":"2023-07-29T06:15:50.154707Z","shell.execute_reply.started":"2023-07-29T06:15:49.060104Z","shell.execute_reply":"2023-07-29T06:15:50.153059Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Import Standard Libs","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n\nfrom keras import metrics # accuracy\nfrom keras import backend as K\n\nimport keras_tuner as kt\n\nimport pandas as pd\nfrom pandas.api.types import is_numeric_dtype\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import log_loss,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom tabpfn import TabPFNClassifier\n\nimport warnings\nfrom tqdm.notebook import tqdm\n\nimport joblib\nimport os\nimport shutil\n\npd.set_option('display.max_rows', 500)\nwarnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:15:50.156516Z","iopub.execute_input":"2023-07-29T06:15:50.157114Z","iopub.status.idle":"2023-07-29T06:16:04.033023Z","shell.execute_reply.started":"2023-07-29T06:15:50.157069Z","shell.execute_reply":"2023-07-29T06:16:04.031881Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the Dataset","metadata":{}},{"cell_type":"code","source":"dataset_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv', index_col='Id')\ndataset_df.columns = dataset_df.columns.str.rstrip()\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))\n\ndataset_test_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv', index_col='Id')\ndataset_test_df.columns = dataset_test_df.columns.str.rstrip()\nprint(\"Full test dataset shape is {}\".format(dataset_test_df.shape))","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:16:04.035682Z","iopub.execute_input":"2023-07-29T06:16:04.036659Z","iopub.status.idle":"2023-07-29T06:16:04.096888Z","shell.execute_reply.started":"2023-07-29T06:16:04.036611Z","shell.execute_reply":"2023-07-29T06:16:04.095773Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Full train dataset shape is (617, 57)\nFull test dataset shape is (5, 56)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Compute Basic Info","metadata":{}},{"cell_type":"code","source":"mining_columns: list = [i for i in dataset_df.columns if i not in [\"Id\",\"Class\"]]\n\ndef compute_basic_stats(columns: list, df: pd.DataFrame) -> pd.DataFrame:\n    \n    out: dict = {}\n    \n    for i in tqdm(columns):\n        mask = df[i].notna()\n        \n        out[i] = {'nunique':df[i].nunique(),\n                  'na_share':round(100*df[i].isna().sum()/df[i].count(),1),\n                  'dtype':df[i].dtype\n                 }\n        if is_numeric_dtype(df[i]):\n            out[i]['correlation'] = round(np.corrcoef(x=df.loc[mask,i],y=df.loc[mask,'Class'])[0,1],2)\n            out[i]['min'] = df.loc[mask,i].min()\n            out[i]['max'] = df.loc[mask,i].max()\n            out[i]['std'] = df.loc[mask,i].std()\n            out[i]['mean'] = df.loc[mask,i].mean()\n            i_lorreg = LogisticRegression()\n            X = df.loc[mask,i].values.reshape(-1,1)\n            y = df.loc[mask,'Class'].values\n            i_lorreg.fit(X=X, y=y)\n            y_pred = i_lorreg.predict(X)\n            out[i]['logloss'] = log_loss(y_true=y, y_pred=y_pred)\n            \n            \n    out = pd.DataFrame(out).T\n    \n    out = out.sort_values('logloss',ascending=True)\n    \n    return out\n\n# Train\nbasic_stats_1 = compute_basic_stats(columns=mining_columns, df=dataset_df)\n\nbasic_stats_1.to_pickle('/kaggle/working/basic_stats_1.pickle')\n\n# Inference\nbasic_stats_1 = pd.read_pickle('/kaggle/working/basic_stats_1.pickle')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:16:14.806347Z","iopub.execute_input":"2023-07-29T06:16:14.806794Z","iopub.status.idle":"2023-07-29T06:16:15.378943Z","shell.execute_reply.started":"2023-07-29T06:16:14.806759Z","shell.execute_reply":"2023-07-29T06:16:15.377839Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/56 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"653fcb52b59743389b7b389adf526e57"}},"metadata":{}}]},{"cell_type":"code","source":"basic_stats_1","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:17:25.185797Z","iopub.execute_input":"2023-07-29T06:17:25.186221Z","iopub.status.idle":"2023-07-29T06:17:25.231161Z","shell.execute_reply.started":"2023-07-29T06:17:25.186188Z","shell.execute_reply":"2023-07-29T06:17:25.230080Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   nunique na_share    dtype correlation          min          max  \\\nDU     253      0.2  float64        0.26     0.005518   161.355315   \nBC     259      0.0  float64        0.16       1.2299  1463.693448   \nAF     599      0.0  float64         0.3    192.59328  28688.18766   \nEH     127      0.0  float64        0.18     0.003042    42.569748   \nAM     605      0.0  float64        0.24     3.177522    630.51823   \nFD     337      0.0  float64        0.13      0.29685  1578.654237   \nDI     571      0.0  float64        0.26     60.23247  1049.168078   \nFR     435      0.0  float64         0.1      0.49706   1244.22702   \nAB     217      0.0  float64        0.28     0.081187     6.161666   \nCF     586      0.0  float64        0.11     0.510888   200.967526   \nBZ     115      0.0  float64        0.11   257.432377   50092.4593   \nFE     615      0.0  float64        0.22  1563.136688  143224.6823   \nAX     427      0.0  float64         0.1     0.699861     38.27088   \nBR     566      0.0  float64        0.09    51.216883  179250.2529   \nGB     560      0.0  float64        0.08     4.102182   135.781294   \nAY     148      0.0  float64        0.08     0.025578    10.315851   \nFC     600      0.2  float64        0.03     7.534128  3030.655824   \nFS     161      0.3  float64         0.0      0.06773    31.365763   \nCC     602      0.5  float64       -0.05     0.176874     4.103032   \nGH     596      0.0  float64        0.03     9.432735    81.210825   \nDV      39      0.0  float64        0.02      1.74307     25.19293   \nDY     590      0.0  float64        0.06     0.804068   152.355164   \nEE     513      0.0  float64       -0.14     0.286201    18.324926   \nGE     264      0.0  float64       -0.07    72.611063  1497.351958   \nAH     227      0.0  float64        0.04    85.200147  1910.123198   \nDN     576      0.0  float64       -0.01     6.339496    62.808096   \nEP     275      0.0  float64       -0.07    78.526968  1063.594578   \nEU     455      0.0  float64       -0.04     3.828384   6501.26448   \nEG     610      0.0  float64       -0.02     185.5941  30243.75878   \nFI     498      0.0  float64       -0.09      3.58345    35.851039   \nDH     191      0.0  float64       -0.21     0.040995     1.060404   \nGF     611      0.0  float64       -0.13    13.038894  143790.0712   \nGI     615      0.0  float64        0.08     0.897628   191.194764   \nDE     616      0.0  float64       -0.12    35.998895   2103.40519   \nDA     611      0.0  float64        -0.2       6.9064    210.33092   \nCW     426      0.0  float64       -0.06      7.03064    64.521624   \nCU     307      0.0  float64       -0.08     0.137925     4.951507   \nAZ     484      0.0  float64        0.01     3.396778    38.971568   \nCL     123      0.0  float64        0.02     1.050225    31.688153   \nCH     135      0.0  float64        0.01     0.003184     0.224074   \nBD     617      0.0  float64        0.11   1693.62432  53060.59924   \nBN      53      0.0  float64         0.2       9.8868      29.3073   \nDL     604      0.0  float64       -0.15      10.3456     326.2362   \nCS     576      0.0  float64       -0.05    13.784111   267.942823   \nFL     388      0.2  float64        0.24     0.173229   137.932739   \nGL     355      0.2  float64       -0.12     0.001129       21.978   \nCB     553      0.3  float64       -0.01     12.49976  2271.436167   \nDF     137      0.0  float64        0.06      0.23868    37.895013   \nBP     612      0.0  float64        0.16    72.948951   2447.81055   \nEB     439      0.0  float64        0.09     4.926396     94.95858   \nAR     130      0.0  float64        0.06     8.138688   178.943634   \nCD     584      0.0  float64        0.17      23.3876   633.534408   \nEL     311     10.8  float64        0.07     5.394675   109.125159   \nCR     595      0.0  float64       -0.23     0.069225     3.039675   \nBQ     515     10.8  float64        0.28     1.331155   344.644105   \nEJ       2      0.0   object         NaN          NaN          NaN   \n\n             std          mean   logloss  \nDU      9.034721        1.8029   5.55868  \nBC     65.166943      8.053012  5.724924  \nAF   2300.322717   3502.013221  5.900177  \nEH      1.847499      0.305107  5.900177  \nAM     69.728226     38.968552  5.900177  \nFD     64.754262      6.930086  5.900177  \nDI     86.084419    146.972099  5.900177  \nFR     50.181948      3.533905  5.900177  \nAB      0.468388      0.477149  6.017012  \nCF     13.571133     11.241064  6.192265  \nBZ   2076.371275    550.632525  6.192265  \nFE  11331.294051  10306.810737  6.192265  \nAX      2.551696      5.545576  6.250682  \nBR   7575.293707   1218.133238  6.250682  \nGB      9.991907     20.724856  6.250682  \nAY      0.416817       0.06032  6.250682  \nFC    165.551545     71.341526  6.260829  \nFS      1.305365      0.421501   6.27101  \nCC      0.263994      0.688801  6.281223  \nGH      9.864239     31.489716    6.3091  \nDV      1.484555       1.92483    6.3091  \nDY     18.116679     26.388989    6.3091  \nEE      2.058344      3.064778    6.3091  \nGE    144.181524    131.714987    6.3091  \nAH     127.83895    118.624513    6.3091  \nDN      8.038825     26.370568    6.3091  \nEP      68.44562    105.060712    6.3091  \nEU    390.187057     69.117005    6.3091  \nEG   1790.227476   1731.248215    6.3091  \nFI      2.934025     10.111079    6.3091  \nDH      0.112989      0.367002    6.3091  \nGF  19352.959387  14679.595398    6.3091  \nGI     36.266251     50.584437    6.3091  \nDE    317.745623    401.901299    6.3091  \nDA     21.210888     51.128326    6.3091  \nCW     14.645993     27.165653    6.3091  \nCU      0.538717      1.383792    6.3091  \nAZ      4.350645     10.566447    6.3091  \nCL       1.92221      1.403761    6.3091  \nCH      0.014808      0.030615    6.3091  \nBD   3021.326641   5350.388655    6.3091  \nBN      3.478278     21.419492    6.3091  \nDL     28.243187     94.795377    6.3091  \nCS     17.266347      36.91759    6.3091  \nFL     11.496257      5.433199  6.319342  \nGL      10.32701      8.530961  6.319342  \nCB    159.049302     77.104151  6.329617  \nDF      1.912384      0.633884  6.367517  \nBP    183.992505    231.322223  6.367517  \nEB      6.200281        9.0727  6.425935  \nAR     10.518877     10.128242  6.425935  \nCD      51.58513     90.251735  6.425935  \nEL     38.555707     69.582596  6.600454  \nCR      0.281195      0.742262  6.601188  \nBQ     96.479371     98.328737  7.053426  \nEJ           NaN           NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nunique</th>\n      <th>na_share</th>\n      <th>dtype</th>\n      <th>correlation</th>\n      <th>min</th>\n      <th>max</th>\n      <th>std</th>\n      <th>mean</th>\n      <th>logloss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>DU</th>\n      <td>253</td>\n      <td>0.2</td>\n      <td>float64</td>\n      <td>0.26</td>\n      <td>0.005518</td>\n      <td>161.355315</td>\n      <td>9.034721</td>\n      <td>1.8029</td>\n      <td>5.55868</td>\n    </tr>\n    <tr>\n      <th>BC</th>\n      <td>259</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.16</td>\n      <td>1.2299</td>\n      <td>1463.693448</td>\n      <td>65.166943</td>\n      <td>8.053012</td>\n      <td>5.724924</td>\n    </tr>\n    <tr>\n      <th>AF</th>\n      <td>599</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.3</td>\n      <td>192.59328</td>\n      <td>28688.18766</td>\n      <td>2300.322717</td>\n      <td>3502.013221</td>\n      <td>5.900177</td>\n    </tr>\n    <tr>\n      <th>EH</th>\n      <td>127</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.18</td>\n      <td>0.003042</td>\n      <td>42.569748</td>\n      <td>1.847499</td>\n      <td>0.305107</td>\n      <td>5.900177</td>\n    </tr>\n    <tr>\n      <th>AM</th>\n      <td>605</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.24</td>\n      <td>3.177522</td>\n      <td>630.51823</td>\n      <td>69.728226</td>\n      <td>38.968552</td>\n      <td>5.900177</td>\n    </tr>\n    <tr>\n      <th>FD</th>\n      <td>337</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.13</td>\n      <td>0.29685</td>\n      <td>1578.654237</td>\n      <td>64.754262</td>\n      <td>6.930086</td>\n      <td>5.900177</td>\n    </tr>\n    <tr>\n      <th>DI</th>\n      <td>571</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.26</td>\n      <td>60.23247</td>\n      <td>1049.168078</td>\n      <td>86.084419</td>\n      <td>146.972099</td>\n      <td>5.900177</td>\n    </tr>\n    <tr>\n      <th>FR</th>\n      <td>435</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.1</td>\n      <td>0.49706</td>\n      <td>1244.22702</td>\n      <td>50.181948</td>\n      <td>3.533905</td>\n      <td>5.900177</td>\n    </tr>\n    <tr>\n      <th>AB</th>\n      <td>217</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.28</td>\n      <td>0.081187</td>\n      <td>6.161666</td>\n      <td>0.468388</td>\n      <td>0.477149</td>\n      <td>6.017012</td>\n    </tr>\n    <tr>\n      <th>CF</th>\n      <td>586</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.11</td>\n      <td>0.510888</td>\n      <td>200.967526</td>\n      <td>13.571133</td>\n      <td>11.241064</td>\n      <td>6.192265</td>\n    </tr>\n    <tr>\n      <th>BZ</th>\n      <td>115</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.11</td>\n      <td>257.432377</td>\n      <td>50092.4593</td>\n      <td>2076.371275</td>\n      <td>550.632525</td>\n      <td>6.192265</td>\n    </tr>\n    <tr>\n      <th>FE</th>\n      <td>615</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.22</td>\n      <td>1563.136688</td>\n      <td>143224.6823</td>\n      <td>11331.294051</td>\n      <td>10306.810737</td>\n      <td>6.192265</td>\n    </tr>\n    <tr>\n      <th>AX</th>\n      <td>427</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.1</td>\n      <td>0.699861</td>\n      <td>38.27088</td>\n      <td>2.551696</td>\n      <td>5.545576</td>\n      <td>6.250682</td>\n    </tr>\n    <tr>\n      <th>BR</th>\n      <td>566</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.09</td>\n      <td>51.216883</td>\n      <td>179250.2529</td>\n      <td>7575.293707</td>\n      <td>1218.133238</td>\n      <td>6.250682</td>\n    </tr>\n    <tr>\n      <th>GB</th>\n      <td>560</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.08</td>\n      <td>4.102182</td>\n      <td>135.781294</td>\n      <td>9.991907</td>\n      <td>20.724856</td>\n      <td>6.250682</td>\n    </tr>\n    <tr>\n      <th>AY</th>\n      <td>148</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.08</td>\n      <td>0.025578</td>\n      <td>10.315851</td>\n      <td>0.416817</td>\n      <td>0.06032</td>\n      <td>6.250682</td>\n    </tr>\n    <tr>\n      <th>FC</th>\n      <td>600</td>\n      <td>0.2</td>\n      <td>float64</td>\n      <td>0.03</td>\n      <td>7.534128</td>\n      <td>3030.655824</td>\n      <td>165.551545</td>\n      <td>71.341526</td>\n      <td>6.260829</td>\n    </tr>\n    <tr>\n      <th>FS</th>\n      <td>161</td>\n      <td>0.3</td>\n      <td>float64</td>\n      <td>0.0</td>\n      <td>0.06773</td>\n      <td>31.365763</td>\n      <td>1.305365</td>\n      <td>0.421501</td>\n      <td>6.27101</td>\n    </tr>\n    <tr>\n      <th>CC</th>\n      <td>602</td>\n      <td>0.5</td>\n      <td>float64</td>\n      <td>-0.05</td>\n      <td>0.176874</td>\n      <td>4.103032</td>\n      <td>0.263994</td>\n      <td>0.688801</td>\n      <td>6.281223</td>\n    </tr>\n    <tr>\n      <th>GH</th>\n      <td>596</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.03</td>\n      <td>9.432735</td>\n      <td>81.210825</td>\n      <td>9.864239</td>\n      <td>31.489716</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>DV</th>\n      <td>39</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.02</td>\n      <td>1.74307</td>\n      <td>25.19293</td>\n      <td>1.484555</td>\n      <td>1.92483</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>DY</th>\n      <td>590</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.06</td>\n      <td>0.804068</td>\n      <td>152.355164</td>\n      <td>18.116679</td>\n      <td>26.388989</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>EE</th>\n      <td>513</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.14</td>\n      <td>0.286201</td>\n      <td>18.324926</td>\n      <td>2.058344</td>\n      <td>3.064778</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>GE</th>\n      <td>264</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.07</td>\n      <td>72.611063</td>\n      <td>1497.351958</td>\n      <td>144.181524</td>\n      <td>131.714987</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>AH</th>\n      <td>227</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.04</td>\n      <td>85.200147</td>\n      <td>1910.123198</td>\n      <td>127.83895</td>\n      <td>118.624513</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>DN</th>\n      <td>576</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.01</td>\n      <td>6.339496</td>\n      <td>62.808096</td>\n      <td>8.038825</td>\n      <td>26.370568</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>EP</th>\n      <td>275</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.07</td>\n      <td>78.526968</td>\n      <td>1063.594578</td>\n      <td>68.44562</td>\n      <td>105.060712</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>EU</th>\n      <td>455</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.04</td>\n      <td>3.828384</td>\n      <td>6501.26448</td>\n      <td>390.187057</td>\n      <td>69.117005</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>EG</th>\n      <td>610</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.02</td>\n      <td>185.5941</td>\n      <td>30243.75878</td>\n      <td>1790.227476</td>\n      <td>1731.248215</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>FI</th>\n      <td>498</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.09</td>\n      <td>3.58345</td>\n      <td>35.851039</td>\n      <td>2.934025</td>\n      <td>10.111079</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>DH</th>\n      <td>191</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.21</td>\n      <td>0.040995</td>\n      <td>1.060404</td>\n      <td>0.112989</td>\n      <td>0.367002</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>GF</th>\n      <td>611</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.13</td>\n      <td>13.038894</td>\n      <td>143790.0712</td>\n      <td>19352.959387</td>\n      <td>14679.595398</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>GI</th>\n      <td>615</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.08</td>\n      <td>0.897628</td>\n      <td>191.194764</td>\n      <td>36.266251</td>\n      <td>50.584437</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>DE</th>\n      <td>616</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.12</td>\n      <td>35.998895</td>\n      <td>2103.40519</td>\n      <td>317.745623</td>\n      <td>401.901299</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>DA</th>\n      <td>611</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.2</td>\n      <td>6.9064</td>\n      <td>210.33092</td>\n      <td>21.210888</td>\n      <td>51.128326</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>CW</th>\n      <td>426</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.06</td>\n      <td>7.03064</td>\n      <td>64.521624</td>\n      <td>14.645993</td>\n      <td>27.165653</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>CU</th>\n      <td>307</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.08</td>\n      <td>0.137925</td>\n      <td>4.951507</td>\n      <td>0.538717</td>\n      <td>1.383792</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>AZ</th>\n      <td>484</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.01</td>\n      <td>3.396778</td>\n      <td>38.971568</td>\n      <td>4.350645</td>\n      <td>10.566447</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>CL</th>\n      <td>123</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.02</td>\n      <td>1.050225</td>\n      <td>31.688153</td>\n      <td>1.92221</td>\n      <td>1.403761</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>CH</th>\n      <td>135</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.01</td>\n      <td>0.003184</td>\n      <td>0.224074</td>\n      <td>0.014808</td>\n      <td>0.030615</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>BD</th>\n      <td>617</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.11</td>\n      <td>1693.62432</td>\n      <td>53060.59924</td>\n      <td>3021.326641</td>\n      <td>5350.388655</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>BN</th>\n      <td>53</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.2</td>\n      <td>9.8868</td>\n      <td>29.3073</td>\n      <td>3.478278</td>\n      <td>21.419492</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>DL</th>\n      <td>604</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.15</td>\n      <td>10.3456</td>\n      <td>326.2362</td>\n      <td>28.243187</td>\n      <td>94.795377</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>CS</th>\n      <td>576</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.05</td>\n      <td>13.784111</td>\n      <td>267.942823</td>\n      <td>17.266347</td>\n      <td>36.91759</td>\n      <td>6.3091</td>\n    </tr>\n    <tr>\n      <th>FL</th>\n      <td>388</td>\n      <td>0.2</td>\n      <td>float64</td>\n      <td>0.24</td>\n      <td>0.173229</td>\n      <td>137.932739</td>\n      <td>11.496257</td>\n      <td>5.433199</td>\n      <td>6.319342</td>\n    </tr>\n    <tr>\n      <th>GL</th>\n      <td>355</td>\n      <td>0.2</td>\n      <td>float64</td>\n      <td>-0.12</td>\n      <td>0.001129</td>\n      <td>21.978</td>\n      <td>10.32701</td>\n      <td>8.530961</td>\n      <td>6.319342</td>\n    </tr>\n    <tr>\n      <th>CB</th>\n      <td>553</td>\n      <td>0.3</td>\n      <td>float64</td>\n      <td>-0.01</td>\n      <td>12.49976</td>\n      <td>2271.436167</td>\n      <td>159.049302</td>\n      <td>77.104151</td>\n      <td>6.329617</td>\n    </tr>\n    <tr>\n      <th>DF</th>\n      <td>137</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.06</td>\n      <td>0.23868</td>\n      <td>37.895013</td>\n      <td>1.912384</td>\n      <td>0.633884</td>\n      <td>6.367517</td>\n    </tr>\n    <tr>\n      <th>BP</th>\n      <td>612</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.16</td>\n      <td>72.948951</td>\n      <td>2447.81055</td>\n      <td>183.992505</td>\n      <td>231.322223</td>\n      <td>6.367517</td>\n    </tr>\n    <tr>\n      <th>EB</th>\n      <td>439</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.09</td>\n      <td>4.926396</td>\n      <td>94.95858</td>\n      <td>6.200281</td>\n      <td>9.0727</td>\n      <td>6.425935</td>\n    </tr>\n    <tr>\n      <th>AR</th>\n      <td>130</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.06</td>\n      <td>8.138688</td>\n      <td>178.943634</td>\n      <td>10.518877</td>\n      <td>10.128242</td>\n      <td>6.425935</td>\n    </tr>\n    <tr>\n      <th>CD</th>\n      <td>584</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>0.17</td>\n      <td>23.3876</td>\n      <td>633.534408</td>\n      <td>51.58513</td>\n      <td>90.251735</td>\n      <td>6.425935</td>\n    </tr>\n    <tr>\n      <th>EL</th>\n      <td>311</td>\n      <td>10.8</td>\n      <td>float64</td>\n      <td>0.07</td>\n      <td>5.394675</td>\n      <td>109.125159</td>\n      <td>38.555707</td>\n      <td>69.582596</td>\n      <td>6.600454</td>\n    </tr>\n    <tr>\n      <th>CR</th>\n      <td>595</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>-0.23</td>\n      <td>0.069225</td>\n      <td>3.039675</td>\n      <td>0.281195</td>\n      <td>0.742262</td>\n      <td>6.601188</td>\n    </tr>\n    <tr>\n      <th>BQ</th>\n      <td>515</td>\n      <td>10.8</td>\n      <td>float64</td>\n      <td>0.28</td>\n      <td>1.331155</td>\n      <td>344.644105</td>\n      <td>96.479371</td>\n      <td>98.328737</td>\n      <td>7.053426</td>\n    </tr>\n    <tr>\n      <th>EJ</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>object</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Only one variable looks constrant over the target -> better to omit it.\n* Realised Better to add LogLoss metric for each feature -> loggloss\n","metadata":{}},{"cell_type":"markdown","source":"# Create Features","metadata":{}},{"cell_type":"code","source":"WoE_Columns = ['DU', 'BC', 'AF', 'EH', 'AM', 'FD', 'DI', 'FR', 'AB', 'CF', 'BZ', 'FE', 'AX', 'BR',\n               'GB', 'AY', 'FC', 'FS', 'CC', 'GH', 'DV', 'DY', 'EE', 'GE', 'AH', 'DN', 'EP', 'EU',\n               'EG', 'FI', 'DH', 'GF', 'GI', 'DE', 'DA', 'CW', 'CU', 'AZ', 'CL', 'CH', 'BD', 'BN',\n               'DL', 'CS', 'FL', 'GL', 'CB', 'DF', 'BP', 'EB', 'AR', 'CD', 'EL', 'CR', 'BQ']","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:17:31.431371Z","iopub.execute_input":"2023-07-29T06:17:31.431767Z","iopub.status.idle":"2023-07-29T06:17:31.439560Z","shell.execute_reply.started":"2023-07-29T06:17:31.431739Z","shell.execute_reply":"2023-07-29T06:17:31.438279Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def preprocess_train(train: pd.DataFrame, numeric_features: list, save_path: str) -> None:\n    woes = dict()\n    \n    # Make WoE Columns\n    for i in tqdm(numeric_features, 'WoE Encoding: '):\n        tmp_woe = WOENumericalComplex()\n        tmp_woe.fit(x=train[i], y=train['Class'])\n        woes[i] = tmp_woe\n    \n    # Save WoE\n    _ = joblib.dump(value=woes, filename=save_path)\n    print('Saved features: ', save_path)\n    \n    return None\n\ndef preprocess_inference(train: pd.DataFrame, test: pd.DataFrame, \n                         numeric_features: list,\n                         stats: pd.DataFrame, save_path: str) -> (pd.DataFrame, pd.DataFrame, list):\n    info = dict()\n    train_out = train.copy()\n    test_out = test.copy()\n    \n    out_features = list()\n    \n    # Make WoE Columns\n    woes = joblib.load(save_path)\n    \n    for i in tqdm(numeric_features, 'WoE Encoding: '):\n        train_out[i + '_WoE'] = woes[i].transform(X=train_out[i])\n        test_out[i + '_WoE'] = woes[i].transform(X=test_out[i])\n        out_features.append(i + '_WoE')\n    \n    # Make NA columns\n    for i in tqdm(['DU', 'FC', 'FS', 'CC', 'FL', 'GL', 'CB', 'EL', 'BQ'], 'Split by NA: '):\n        train_out[i+'_na'] = np.where(train_out[i].isna(),1,0)\n        test_out[i+'_na'] = np.where(test_out[i].isna(),1,0)\n        out_features.append(i + '_na')\n    \n    # Basic Logic -> normalise\n    for i in tqdm(numeric_features,'Normalise Numeric: '):\n        if stats.loc[i,'correlation'] > 0:\n            na_value = stats.loc[i,'max']\n        else:\n            na_value = stats.loc[i,'min']\n\n        train_out[i] = train_out[i].fillna(na_value)\n        test_out[i] = test_out[i].fillna(na_value)\n\n        train_out[i] = (train_out[i]-stats.loc[i,'mean'])/stats.loc[i,'std']\n        test_out[i] = (test_out[i]-stats.loc[i,'mean'])/stats.loc[i,'std']\n\n        out_features.append(i)\n            \n            \n    # Addition EJ -> has only two values, so if EJ == 'A'\n    train_out['EJ' + '_A'] = np.where(train_out['EJ'] == 'A',1,0)\n    test_out['EJ' + '_A'] = np.where(test_out['EJ'] == 'A',1,0)\n    out_features.append('EJ' + '_A')\n    \n    return train_out,test_out,out_features\n\n\n# Only for Train\n_ = preprocess_train(train=dataset_df, numeric_features=WoE_Columns, save_path='/kaggle/working/WoE.pickle')\n\n# For Train & Inference\ntrain_out,test_out,features = preprocess_inference(train=dataset_df, \n                                                   test=dataset_test_df,\n                                                   stats=basic_stats_1, \n                                                   numeric_features=WoE_Columns,\n                                                   save_path='/kaggle/working/WoE.pickle')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:18:22.558062Z","iopub.execute_input":"2023-07-29T06:18:22.558879Z","iopub.status.idle":"2023-07-29T06:18:47.232468Z","shell.execute_reply.started":"2023-07-29T06:18:22.558844Z","shell.execute_reply":"2023-07-29T06:18:47.231323Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"WoE Encoding:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd54207e432841ffbef29be2559f416e"}},"metadata":{}},{"name":"stdout","text":"Saved features:  /kaggle/working/WoE.pickle\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"WoE Encoding:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0b53596c8d46b7aa82ae105b0d4eac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Split by NA:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf77b6a485749dd9b939dfb328bc08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Normalise Numeric:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de7e432c255740cbbff1d2b3394aac97"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Train Model\n\nToday, we will use the defaults to create the Random Forest Model. By default the model is set to train for a classification task.\nWe will train a model for each fold and after training we will store the model and metrics. Here, we have chosen `accuracy` and `binary_crossentropy` as the metrics.","metadata":{}},{"cell_type":"code","source":"class BalancedLogLoss(tf.keras.metrics.Metric):\n    def __init__(self, name='balanced_log_loss', **kwargs):\n        super(BalancedLogLoss, self).__init__(name=name, **kwargs)\n        self.log_loss = self.add_weight(name='log_loss', initializer='zeros')\n\n    def update_state(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight=None):\n        \n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        \n        # Correct Values\n        min_val = 1e-15\n        max_val = 0.999999999999999\n        \n        y_pred = tf.math.minimum(y_pred, [max_val])\n        y_pred = tf.math.maximum(y_pred, [min_val])\n        \n        log_y_pred_1 = tf.reshape(K.log(y_pred),[-1,1])\n        log_y_pred_0 = tf.reshape(K.log(1-y_pred),[-1,1])\n\n        y_1 = tf.reshape(y_true,[1,-1])\n        y_0 = 1-y_1\n\n        logloss_1 = -K.dot(y_1,log_y_pred_1)[0][0]/K.sum(y_1)\n        logloss_0 = -K.dot(y_0,log_y_pred_0)[0][0]/K.sum(y_0)\n\n        av_logloss = (logloss_1+logloss_0)/2\n        \n        self.log_loss.assign_add(av_logloss)\n\n    def result(self):\n        return self.log_loss\n\n    def reset_state(self):\n        # The state of the metric will be reset at the start of each epoch.\n        self.log_loss.assign(0.)\n\ndef plot_train_logs(model) -> None:\n\n    logs = model.make_inspector().training_logs()\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Accuracy (out-of-bag)\")\n\n    plt.subplot(1, 2, 2)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Logloss (out-of-bag)\")\n\n    plt.show()\n\n\nfrom sklearn.base import BaseEstimator \nfrom sklearn.base import RegressorMixin\n    \nclass TFDF_CV_Ensemble(RegressorMixin,BaseEstimator):\n    def __init__(self, model_obj = tfdf.keras.RandomForestModel, label = \"Class\"):\n        self.label: str = label\n        self.model_obj = model_obj\n        \n        # Empty\n        self.X_summary: pd.DataFrame = pd.DataFrame()\n        self.valid_summary: pd.DataFrame = pd.DataFrame()\n        self.features: list = list()\n        self.models: dict = dict()\n        self.metrics: dict = dict()\n        \n        \n    def _compute_weights(self, df: pd.DataFrame) -> dict:\n        # Calculate the number of samples for each label.\n        neg, pos = np.bincount(df[self.label])\n        total = neg + pos\n        weight_for_0 = (1 / neg) * (total / 2.0)\n        weight_for_1 = (1 / pos) * (total / 2.0)\n        class_weight = {0: weight_for_0, 1: weight_for_1}\n        \n        return class_weight\n        \n    def fit(self, X: pd.DataFrame, features: list, splitter = StratifiedKFold(),\n            model_kwargs = dict(), model_compile_kwargs = dict()):\n\n        n_splits = splitter.get_n_splits()\n\n        # Create a various frames\n        self.X_summary = pd.DataFrame(data=np.full((len(X.index),n_splits), np.nan), index=X.index) # For In-Sample Predictions of each Fold\n        self.valid_summary = pd.DataFrame(data=np.full((len(X.index),1), np.nan), index=X.index) # For Out-of-Sample Prediction of each Fold\n        self.features: list = features\n        \n        # Create an empty dictionary to store the models Xed for each fold.\n        self.models = {}\n        self.metrics = {}\n        balanced_logloss_train = {}\n        balanced_logloss_val = {}\n\n        class_weight: dict = self._compute_weights(X)\n        \n        for i, (train_index, valid_index) in enumerate(splitter.split(X=X,y=X['Class'])):\n                print('##### Fold',i+1)\n\n                # Fetch values corresponding to the index \n                train_df = X.iloc[train_index]\n                valid_df = X.iloc[valid_index]\n                valid_ids = valid_df.index.values\n                train_ids = train_df.index.values\n\n                # Select only feature columns for training.\n                train_df = train_df[self.features+[self.label]]\n                valid_df = valid_df[self.features+[self.label]]\n\n                # We need to convert the datatset from Pandas format (pd.DataFrame)\n                train_tf = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=self.label)\n                valid_tf = tfdf.keras.pd_dataframe_to_tf_dataset(valid_df, label=self.label)\n\n                # Define & Train the model and metrics\n                model = self.model_obj(**model_kwargs)\n                model.compile(**model_compile_kwargs) \n                model.fit(x=train_tf, class_weight=class_weight)\n\n                # Store the model\n                self.models[i] = model\n\n                # Predict Values\n                self.X_summary.loc[train_ids, i] = model.predict(x=train_tf).flatten()\n                self.valid_summary.loc[valid_ids, 0] = model.predict(x=valid_tf).flatten()\n\n                # Evaluate and store the metrics in respective dicts\n                evaluation = model.evaluate(x=train_tf,return_dict=True)\n                train_metric = evaluation[\"balanced_log_loss\"]\n\n                evaluation = model.evaluate(x=valid_tf,return_dict=True)\n                val_metric = evaluation[\"balanced_log_loss\"]\n\n                # Plot Results\n                plot_train_logs(model)\n\n                balanced_logloss_train[i] = train_metric\n                balanced_logloss_val[i] = val_metric\n\n                print(f\"\\nTrain: {train_metric:.4f} Validation: {val_metric:.4f}\")\n\n        self.metrics['train'] = balanced_logloss_train\n        self.metrics['val'] = balanced_logloss_val\n\n        print(f\"\\nTrain mean: {pd.Series(self.metrics['train']).mean():.4f} std: {pd.Series(self.metrics['train']).std():.4f}\")\n        print(f\"\\nValidation mean: {pd.Series(self.metrics['val']).mean():.4f} std: {pd.Series(self.metrics['val']).std():.4f}\")\n        \n        return self\n    \n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        n_splits = len(self.models)\n        X_tf = tfdf.keras.pd_dataframe_to_tf_dataset(X[self.features]) # Technial, to conert frame to tensor for makind a predictions using tensor framework\n        X_summary = pd.DataFrame(data=np.full((len(X.index),n_splits), np.nan),index=X.index) # For X (Sumbition) Predictions of each Fold's Model\n\n        for i, model in enumerate(self.models.values()):\n            X_summary[i] = model.predict(x=X_tf).flatten() \n        \n        return X_summary\n    \n    def save(self, save_path: str) -> None:\n        try:\n            shutil.rmtree(save_path)\n        except FileNotFoundError:\n            pass\n        else:\n            pass\n            \n        os.makedirs(f'{save_path}/models', exist_ok=True)\n        \n        for fold, model in self.models.items():\n            model.save(filepath=f'{save_path}/models/{fold}')\n        \n        # TODO: Other attributes\n        joblib.dump(value=self.label, filename=f'{save_path}/label.pickle')\n        joblib.dump(value=self.model_obj, filename=f'{save_path}/model_obj.pickle')\n        \n        joblib.dump(value=self.X_summary, filename=f'{save_path}/X_summary.pickle')\n        joblib.dump(value=self.valid_summary, filename=f'{save_path}/valid_summary.pickle')\n        joblib.dump(value=self.features, filename=f'{save_path}/features.pickle')\n        joblib.dump(value=self.metrics, filename=f'{save_path}/metrics.pickle')\n            \n        return None\n    \n    def load(self, save_path: str):\n        \n        # TODO: Other attributes\n        self.label = joblib.load(filename=f'{save_path}/label.pickle')\n        self.model_obj = joblib.load(filename=f'{save_path}/model_obj.pickle')\n        \n        self.X_summary = joblib.load(filename=f'{save_path}/X_summary.pickle')\n        self.valid_summary = joblib.load(filename=f'{save_path}/valid_summary.pickle')\n        self.features = joblib.load(filename=f'{save_path}/features.pickle')\n        self.metrics = joblib.load(filename=f'{save_path}/metrics.pickle')\n        \n        self.models = dict()\n        \n        for name in os.listdir(f'{save_path}/models'):\n            i = name.split('.')[0]\n            self.models[int(i)] = tf.keras.models.load_model(f'{save_path}/models/{i}',\n                                              custom_objects={\"BalancedLogLoss\": BalancedLogLoss})\n            \n        return self","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:41:51.612406Z","iopub.execute_input":"2023-07-29T06:41:51.612848Z","iopub.status.idle":"2023-07-29T06:41:51.660500Z","shell.execute_reply.started":"2023-07-29T06:41:51.612797Z","shell.execute_reply":"2023-07-29T06:41:51.659431Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Test\nmy_splitter = StratifiedKFold(n_splits=2,shuffle=True, random_state=1902)\n\n# initialise\nCV_Ensemble_1 = TFDF_CV_Ensemble(model_obj=tfdf.keras.RandomForestModel, label=\"Class\")\n\n# train\nCV_Ensemble_1 = CV_Ensemble_1.fit(X=train_out, features=features, \n                                  splitter=my_splitter,\n                                  model_kwargs=dict(max_depth=6, num_trees=1000),\n                                  model_compile_kwargs=dict(metrics=[metrics.binary_accuracy,BalancedLogLoss()]))\n\n# save\nCV_Ensemble_1.save(save_path='/kaggle/working/RF/1')\n\n# load\nCV_Ensemble_1_load = TFDF_CV_Ensemble()\n\nCV_Ensemble_1_load = CV_Ensemble_1_load.load(save_path='/kaggle/working/RF/1')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:41:52.719087Z","iopub.execute_input":"2023-07-29T06:41:52.719479Z","iopub.status.idle":"2023-07-29T06:42:21.179221Z","shell.execute_reply.started":"2023-07-29T06:41:52.719450Z","shell.execute_reply":"2023-07-29T06:42:21.178021Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"##### Fold 1\nUse /tmp/tmp6cfh3prb as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:01.884086. Found 308 examples.\nTraining model...\nModel trained in 0:00:00.461650\nCompiling model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO 23-07-29 06:41:56.4417 UTC kernel.cc:1242] Loading model from path /tmp/tmp6cfh3prb/model/ with prefix 4e82cc1337254ffa\n[INFO 23-07-29 06:41:56.5398 UTC decision_forest.cc:660] Model loaded with 1000 root(s), 25702 node(s), and 113 input feature(s).\n[INFO 23-07-29 06:41:56.5400 UTC abstract_model.cc:1311] Engine \"RandomForestOptPred\" built\n[INFO 23-07-29 06:41:56.5400 UTC kernel.cc:1074] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model compiled.\n1/1 [==============================] - 0s 270ms/step\n1/1 [==============================] - 0s 288ms/step\n1/1 [==============================] - 1s 607ms/step - loss: 0.0000e+00 - binary_accuracy: 0.9935 - balanced_log_loss: 0.1899\n1/1 [==============================] - 0s 393ms/step - loss: 0.0000e+00 - binary_accuracy: 0.9320 - balanced_log_loss: 0.3693\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\nTrain: 0.1899 Validation: 0.3693\n##### Fold 2\nUse /tmp/tmp0utve8g5 as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:01.965332. Found 309 examples.\nTraining model...\nModel trained in 0:00:00.470664\nCompiling model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO 23-07-29 06:42:04.3933 UTC kernel.cc:1242] Loading model from path /tmp/tmp0utve8g5/model/ with prefix 6434b472766647f2\n[INFO 23-07-29 06:42:04.4945 UTC decision_forest.cc:660] Model loaded with 1000 root(s), 25796 node(s), and 113 input feature(s).\n[INFO 23-07-29 06:42:04.4946 UTC kernel.cc:1074] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model compiled.\n1/1 [==============================] - 0s 268ms/step\n1/1 [==============================] - 0s 275ms/step\n1/1 [==============================] - 1s 592ms/step - loss: 0.0000e+00 - binary_accuracy: 0.9806 - balanced_log_loss: 0.5218\n1/1 [==============================] - 0s 392ms/step - loss: 0.0000e+00 - binary_accuracy: 0.9188 - balanced_log_loss: 0.4127\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\nTrain: 0.5218 Validation: 0.4127\n\nTrain mean: 0.3559 std: 0.2347\n\nValidation mean: 0.3910 std: 0.0307\n","output_type":"stream"},{"name":"stderr","text":"[INFO 23-07-29 06:42:19.9995 UTC kernel.cc:1242] Loading model from path /kaggle/working/RF/1/models/1/assets/ with prefix 6434b472766647f2\n[INFO 23-07-29 06:42:20.0919 UTC decision_forest.cc:660] Model loaded with 1000 root(s), 25796 node(s), and 113 input feature(s).\n[INFO 23-07-29 06:42:20.0920 UTC abstract_model.cc:1311] Engine \"RandomForestOptPred\" built\n[INFO 23-07-29 06:42:20.0921 UTC kernel.cc:1074] Use fast generic engine\n[INFO 23-07-29 06:42:21.0325 UTC kernel.cc:1242] Loading model from path /kaggle/working/RF/1/models/0/assets/ with prefix 4e82cc1337254ffa\n[INFO 23-07-29 06:42:21.1226 UTC decision_forest.cc:660] Model loaded with 1000 root(s), 25702 node(s), and 113 input feature(s).\n[INFO 23-07-29 06:42:21.1229 UTC kernel.cc:1074] Use fast generic engine\n","output_type":"stream"}]},{"cell_type":"code","source":"CV_Ensemble_1_load","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:42:33.564920Z","iopub.execute_input":"2023-07-29T06:42:33.565367Z","iopub.status.idle":"2023-07-29T06:42:33.573147Z","shell.execute_reply.started":"2023-07-29T06:42:33.565332Z","shell.execute_reply":"2023-07-29T06:42:33.572070Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"TFDF_CV_Ensemble()","text/html":"<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TFDF_CV_Ensemble()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TFDF_CV_Ensemble</label><div class=\"sk-toggleable__content\"><pre>TFDF_CV_Ensemble()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"test_summary_rf_1","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:31:16.418551Z","iopub.execute_input":"2023-07-29T06:31:16.419020Z","iopub.status.idle":"2023-07-29T06:31:16.431720Z","shell.execute_reply.started":"2023-07-29T06:31:16.418972Z","shell.execute_reply":"2023-07-29T06:31:16.430559Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                0    1\nId                    \n00eed32682bb  0.0  0.0\n010ebe33f668  0.0  0.0\n02fa521e1838  0.0  0.0\n040e15f562a2  0.0  0.0\n046e85c7cc7f  0.0  0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>00eed32682bb</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>010ebe33f668</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>02fa521e1838</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>040e15f562a2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>046e85c7cc7f</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"reconstructed_model = tf.keras.models.load_model(\"/kaggle/working/RF/1/models/1/\",\n                                                custom_objects={\"BalancedLogLoss\": BalancedLogLoss}) ","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:39:20.525105Z","iopub.execute_input":"2023-07-29T06:39:20.525611Z","iopub.status.idle":"2023-07-29T06:39:21.606480Z","shell.execute_reply.started":"2023-07-29T06:39:20.525569Z","shell.execute_reply":"2023-07-29T06:39:21.604892Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"[INFO 23-07-29 06:39:21.4503 UTC kernel.cc:1242] Loading model from path /kaggle/working/RF/1/models/1/assets/ with prefix 47ebcac6df284f4a\n[INFO 23-07-29 06:39:21.5483 UTC decision_forest.cc:660] Model loaded with 1000 root(s), 25796 node(s), and 113 input feature(s).\n[INFO 23-07-29 06:39:21.5486 UTC abstract_model.cc:1311] Engine \"RandomForestOptPred\" built\n[INFO 23-07-29 06:39:21.5487 UTC kernel.cc:1074] Use fast generic engine\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_summary_rf_3, valid_summary_rf_3, test_summary_rf_3, model_rf_3,metrics_rf_3 \n\ntrain_summary_rf_1 = CV_Ensemble_1_load.X_summary\nvalid_summary_rf_1 = CV_Ensemble_1_load.valid_summary\ntest_summary_rf_1 = CV_Ensemble_1_load.predict(X=test_out)\nmodel_rf_1 = CV_Ensemble_1_load.models\nmetrics_rf_1 = CV_Ensemble_1_load.metrics","metadata":{"execution":{"iopub.status.busy":"2023-07-29T06:30:05.982023Z","iopub.execute_input":"2023-07-29T06:30:05.982424Z","iopub.status.idle":"2023-07-29T06:30:06.751678Z","shell.execute_reply.started":"2023-07-29T06:30:05.982394Z","shell.execute_reply":"2023-07-29T06:30:06.750856Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Warning: The model was called directly (i.e. using `model(data)` instead of using `model.predict(data)`) before being trained. The model will only return zeros until trained. The output shape might change after training {'DU_WoE': <tf.Tensor 'inputs:0' shape=(None,) dtype=float32>, 'BC_WoE': <tf.Tensor 'inputs_1:0' shape=(None,) dtype=float32>, 'AF_WoE': <tf.Tensor 'inputs_2:0' shape=(None,) dtype=float32>, 'EH_WoE': <tf.Tensor 'inputs_3:0' shape=(None,) dtype=float32>, 'AM_WoE': <tf.Tensor 'inputs_4:0' shape=(None,) dtype=float32>, 'FD_WoE': <tf.Tensor 'inputs_5:0' shape=(None,) dtype=float32>, 'DI_WoE': <tf.Tensor 'inputs_6:0' shape=(None,) dtype=float32>, 'FR_WoE': <tf.Tensor 'inputs_7:0' shape=(None,) dtype=float32>, 'AB_WoE': <tf.Tensor 'inputs_8:0' shape=(None,) dtype=float32>, 'CF_WoE': <tf.Tensor 'inputs_9:0' shape=(None,) dtype=float32>, 'BZ_WoE': <tf.Tensor 'inputs_10:0' shape=(None,) dtype=float32>, 'FE_WoE': <tf.Tensor 'inputs_11:0' shape=(None,) dtype=float32>, 'AX_WoE': <tf.Tensor 'inputs_12:0' shape=(None,) dtype=float32>, 'BR_WoE': <tf.Tensor 'inputs_13:0' shape=(None,) dtype=float32>, 'GB_WoE': <tf.Tensor 'inputs_14:0' shape=(None,) dtype=float32>, 'AY_WoE': <tf.Tensor 'inputs_15:0' shape=(None,) dtype=float32>, 'FC_WoE': <tf.Tensor 'inputs_16:0' shape=(None,) dtype=float32>, 'FS_WoE': <tf.Tensor 'inputs_17:0' shape=(None,) dtype=float32>, 'CC_WoE': <tf.Tensor 'inputs_18:0' shape=(None,) dtype=float32>, 'GH_WoE': <tf.Tensor 'inputs_19:0' shape=(None,) dtype=float32>, 'DV_WoE': <tf.Tensor 'inputs_20:0' shape=(None,) dtype=float32>, 'DY_WoE': <tf.Tensor 'inputs_21:0' shape=(None,) dtype=float32>, 'EE_WoE': <tf.Tensor 'inputs_22:0' shape=(None,) dtype=float32>, 'GE_WoE': <tf.Tensor 'inputs_23:0' shape=(None,) dtype=float32>, 'AH_WoE': <tf.Tensor 'inputs_24:0' shape=(None,) dtype=float32>, 'DN_WoE': <tf.Tensor 'inputs_25:0' shape=(None,) dtype=float32>, 'EP_WoE': <tf.Tensor 'inputs_26:0' shape=(None,) dtype=float32>, 'EU_WoE': <tf.Tensor 'inputs_27:0' shape=(None,) dtype=float32>, 'EG_WoE': <tf.Tensor 'inputs_28:0' shape=(None,) dtype=float32>, 'FI_WoE': <tf.Tensor 'inputs_29:0' shape=(None,) dtype=float32>, 'DH_WoE': <tf.Tensor 'inputs_30:0' shape=(None,) dtype=float32>, 'GF_WoE': <tf.Tensor 'inputs_31:0' shape=(None,) dtype=float32>, 'GI_WoE': <tf.Tensor 'inputs_32:0' shape=(None,) dtype=float32>, 'DE_WoE': <tf.Tensor 'inputs_33:0' shape=(None,) dtype=float32>, 'DA_WoE': <tf.Tensor 'inputs_34:0' shape=(None,) dtype=float32>, 'CW_WoE': <tf.Tensor 'inputs_35:0' shape=(None,) dtype=float32>, 'CU_WoE': <tf.Tensor 'inputs_36:0' shape=(None,) dtype=float32>, 'AZ_WoE': <tf.Tensor 'inputs_37:0' shape=(None,) dtype=float32>, 'CL_WoE': <tf.Tensor 'inputs_38:0' shape=(None,) dtype=float32>, 'CH_WoE': <tf.Tensor 'inputs_39:0' shape=(None,) dtype=float32>, 'BD_WoE': <tf.Tensor 'inputs_40:0' shape=(None,) dtype=float32>, 'BN_WoE': <tf.Tensor 'inputs_41:0' shape=(None,) dtype=float32>, 'DL_WoE': <tf.Tensor 'inputs_42:0' shape=(None,) dtype=float32>, 'CS_WoE': <tf.Tensor 'inputs_43:0' shape=(None,) dtype=float32>, 'FL_WoE': <tf.Tensor 'inputs_44:0' shape=(None,) dtype=float32>, 'GL_WoE': <tf.Tensor 'inputs_45:0' shape=(None,) dtype=float32>, 'CB_WoE': <tf.Tensor 'inputs_46:0' shape=(None,) dtype=float32>, 'DF_WoE': <tf.Tensor 'inputs_47:0' shape=(None,) dtype=float32>, 'BP_WoE': <tf.Tensor 'inputs_48:0' shape=(None,) dtype=float32>, 'EB_WoE': <tf.Tensor 'inputs_49:0' shape=(None,) dtype=float32>, 'AR_WoE': <tf.Tensor 'inputs_50:0' shape=(None,) dtype=float32>, 'CD_WoE': <tf.Tensor 'inputs_51:0' shape=(None,) dtype=float32>, 'EL_WoE': <tf.Tensor 'inputs_52:0' shape=(None,) dtype=float32>, 'CR_WoE': <tf.Tensor 'inputs_53:0' shape=(None,) dtype=float32>, 'BQ_WoE': <tf.Tensor 'inputs_54:0' shape=(None,) dtype=float32>, 'DU_na': <tf.Tensor 'inputs_55:0' shape=(None,) dtype=int64>, 'FC_na': <tf.Tensor 'inputs_56:0' shape=(None,) dtype=int64>, 'FS_na': <tf.Tensor 'inputs_57:0' shape=(None,) dtype=int64>, 'CC_na': <tf.Tensor 'inputs_58:0' shape=(None,) dtype=int64>, 'FL_na': <tf.Tensor 'inputs_59:0' shape=(None,) dtype=int64>, 'GL_na': <tf.Tensor 'inputs_60:0' shape=(None,) dtype=int64>, 'CB_na': <tf.Tensor 'inputs_61:0' shape=(None,) dtype=int64>, 'EL_na': <tf.Tensor 'inputs_62:0' shape=(None,) dtype=int64>, 'BQ_na': <tf.Tensor 'inputs_63:0' shape=(None,) dtype=int64>, 'DU': <tf.Tensor 'inputs_64:0' shape=(None,) dtype=float32>, 'BC': <tf.Tensor 'inputs_65:0' shape=(None,) dtype=float32>, 'AF': <tf.Tensor 'inputs_66:0' shape=(None,) dtype=float32>, 'EH': <tf.Tensor 'inputs_67:0' shape=(None,) dtype=float32>, 'AM': <tf.Tensor 'inputs_68:0' shape=(None,) dtype=float32>, 'FD': <tf.Tensor 'inputs_69:0' shape=(None,) dtype=float32>, 'DI': <tf.Tensor 'inputs_70:0' shape=(None,) dtype=float32>, 'FR': <tf.Tensor 'inputs_71:0' shape=(None,) dtype=float32>, 'AB': <tf.Tensor 'inputs_72:0' shape=(None,) dtype=float32>, 'CF': <tf.Tensor 'inputs_73:0' shape=(None,) dtype=float32>, 'BZ': <tf.Tensor 'inputs_74:0' shape=(None,) dtype=float32>, 'FE': <tf.Tensor 'inputs_75:0' shape=(None,) dtype=float32>, 'AX': <tf.Tensor 'inputs_76:0' shape=(None,) dtype=float32>, 'BR': <tf.Tensor 'inputs_77:0' shape=(None,) dtype=float32>, 'GB': <tf.Tensor 'inputs_78:0' shape=(None,) dtype=float32>, 'AY': <tf.Tensor 'inputs_79:0' shape=(None,) dtype=float32>, 'FC': <tf.Tensor 'inputs_80:0' shape=(None,) dtype=float32>, 'FS': <tf.Tensor 'inputs_81:0' shape=(None,) dtype=float32>, 'CC': <tf.Tensor 'inputs_82:0' shape=(None,) dtype=float32>, 'GH': <tf.Tensor 'inputs_83:0' shape=(None,) dtype=float32>, 'DV': <tf.Tensor 'inputs_84:0' shape=(None,) dtype=float32>, 'DY': <tf.Tensor 'inputs_85:0' shape=(None,) dtype=float32>, 'EE': <tf.Tensor 'inputs_86:0' shape=(None,) dtype=float32>, 'GE': <tf.Tensor 'inputs_87:0' shape=(None,) dtype=float32>, 'AH': <tf.Tensor 'inputs_88:0' shape=(None,) dtype=float32>, 'DN': <tf.Tensor 'inputs_89:0' shape=(None,) dtype=float32>, 'EP': <tf.Tensor 'inputs_90:0' shape=(None,) dtype=float32>, 'EU': <tf.Tensor 'inputs_91:0' shape=(None,) dtype=float32>, 'EG': <tf.Tensor 'inputs_92:0' shape=(None,) dtype=float32>, 'FI': <tf.Tensor 'inputs_93:0' shape=(None,) dtype=float32>, 'DH': <tf.Tensor 'inputs_94:0' shape=(None,) dtype=float32>, 'GF': <tf.Tensor 'inputs_95:0' shape=(None,) dtype=float32>, 'GI': <tf.Tensor 'inputs_96:0' shape=(None,) dtype=float32>, 'DE': <tf.Tensor 'inputs_97:0' shape=(None,) dtype=float32>, 'DA': <tf.Tensor 'inputs_98:0' shape=(None,) dtype=float32>, 'CW': <tf.Tensor 'inputs_99:0' shape=(None,) dtype=float32>, 'CU': <tf.Tensor 'inputs_100:0' shape=(None,) dtype=float32>, 'AZ': <tf.Tensor 'inputs_101:0' shape=(None,) dtype=float32>, 'CL': <tf.Tensor 'inputs_102:0' shape=(None,) dtype=float32>, 'CH': <tf.Tensor 'inputs_103:0' shape=(None,) dtype=float32>, 'BD': <tf.Tensor 'inputs_104:0' shape=(None,) dtype=float32>, 'BN': <tf.Tensor 'inputs_105:0' shape=(None,) dtype=float32>, 'DL': <tf.Tensor 'inputs_106:0' shape=(None,) dtype=float32>, 'CS': <tf.Tensor 'inputs_107:0' shape=(None,) dtype=float32>, 'FL': <tf.Tensor 'inputs_108:0' shape=(None,) dtype=float32>, 'GL': <tf.Tensor 'inputs_109:0' shape=(None,) dtype=float32>, 'CB': <tf.Tensor 'inputs_110:0' shape=(None,) dtype=float32>, 'DF': <tf.Tensor 'inputs_111:0' shape=(None,) dtype=float32>, 'BP': <tf.Tensor 'inputs_112:0' shape=(None,) dtype=float32>, 'EB': <tf.Tensor 'inputs_113:0' shape=(None,) dtype=float32>, 'AR': <tf.Tensor 'inputs_114:0' shape=(None,) dtype=float32>, 'CD': <tf.Tensor 'inputs_115:0' shape=(None,) dtype=float32>, 'EL': <tf.Tensor 'inputs_116:0' shape=(None,) dtype=float32>, 'CR': <tf.Tensor 'inputs_117:0' shape=(None,) dtype=float32>, 'BQ': <tf.Tensor 'inputs_118:0' shape=(None,) dtype=float32>, 'EJ_A': <tf.Tensor 'inputs_119:0' shape=(None,) dtype=int64>}\n1/1 [==============================] - 0s 426ms/step\nWarning: The model was called directly (i.e. using `model(data)` instead of using `model.predict(data)`) before being trained. The model will only return zeros until trained. The output shape might change after training {'DU_WoE': <tf.Tensor 'inputs:0' shape=(None,) dtype=float32>, 'BC_WoE': <tf.Tensor 'inputs_1:0' shape=(None,) dtype=float32>, 'AF_WoE': <tf.Tensor 'inputs_2:0' shape=(None,) dtype=float32>, 'EH_WoE': <tf.Tensor 'inputs_3:0' shape=(None,) dtype=float32>, 'AM_WoE': <tf.Tensor 'inputs_4:0' shape=(None,) dtype=float32>, 'FD_WoE': <tf.Tensor 'inputs_5:0' shape=(None,) dtype=float32>, 'DI_WoE': <tf.Tensor 'inputs_6:0' shape=(None,) dtype=float32>, 'FR_WoE': <tf.Tensor 'inputs_7:0' shape=(None,) dtype=float32>, 'AB_WoE': <tf.Tensor 'inputs_8:0' shape=(None,) dtype=float32>, 'CF_WoE': <tf.Tensor 'inputs_9:0' shape=(None,) dtype=float32>, 'BZ_WoE': <tf.Tensor 'inputs_10:0' shape=(None,) dtype=float32>, 'FE_WoE': <tf.Tensor 'inputs_11:0' shape=(None,) dtype=float32>, 'AX_WoE': <tf.Tensor 'inputs_12:0' shape=(None,) dtype=float32>, 'BR_WoE': <tf.Tensor 'inputs_13:0' shape=(None,) dtype=float32>, 'GB_WoE': <tf.Tensor 'inputs_14:0' shape=(None,) dtype=float32>, 'AY_WoE': <tf.Tensor 'inputs_15:0' shape=(None,) dtype=float32>, 'FC_WoE': <tf.Tensor 'inputs_16:0' shape=(None,) dtype=float32>, 'FS_WoE': <tf.Tensor 'inputs_17:0' shape=(None,) dtype=float32>, 'CC_WoE': <tf.Tensor 'inputs_18:0' shape=(None,) dtype=float32>, 'GH_WoE': <tf.Tensor 'inputs_19:0' shape=(None,) dtype=float32>, 'DV_WoE': <tf.Tensor 'inputs_20:0' shape=(None,) dtype=float32>, 'DY_WoE': <tf.Tensor 'inputs_21:0' shape=(None,) dtype=float32>, 'EE_WoE': <tf.Tensor 'inputs_22:0' shape=(None,) dtype=float32>, 'GE_WoE': <tf.Tensor 'inputs_23:0' shape=(None,) dtype=float32>, 'AH_WoE': <tf.Tensor 'inputs_24:0' shape=(None,) dtype=float32>, 'DN_WoE': <tf.Tensor 'inputs_25:0' shape=(None,) dtype=float32>, 'EP_WoE': <tf.Tensor 'inputs_26:0' shape=(None,) dtype=float32>, 'EU_WoE': <tf.Tensor 'inputs_27:0' shape=(None,) dtype=float32>, 'EG_WoE': <tf.Tensor 'inputs_28:0' shape=(None,) dtype=float32>, 'FI_WoE': <tf.Tensor 'inputs_29:0' shape=(None,) dtype=float32>, 'DH_WoE': <tf.Tensor 'inputs_30:0' shape=(None,) dtype=float32>, 'GF_WoE': <tf.Tensor 'inputs_31:0' shape=(None,) dtype=float32>, 'GI_WoE': <tf.Tensor 'inputs_32:0' shape=(None,) dtype=float32>, 'DE_WoE': <tf.Tensor 'inputs_33:0' shape=(None,) dtype=float32>, 'DA_WoE': <tf.Tensor 'inputs_34:0' shape=(None,) dtype=float32>, 'CW_WoE': <tf.Tensor 'inputs_35:0' shape=(None,) dtype=float32>, 'CU_WoE': <tf.Tensor 'inputs_36:0' shape=(None,) dtype=float32>, 'AZ_WoE': <tf.Tensor 'inputs_37:0' shape=(None,) dtype=float32>, 'CL_WoE': <tf.Tensor 'inputs_38:0' shape=(None,) dtype=float32>, 'CH_WoE': <tf.Tensor 'inputs_39:0' shape=(None,) dtype=float32>, 'BD_WoE': <tf.Tensor 'inputs_40:0' shape=(None,) dtype=float32>, 'BN_WoE': <tf.Tensor 'inputs_41:0' shape=(None,) dtype=float32>, 'DL_WoE': <tf.Tensor 'inputs_42:0' shape=(None,) dtype=float32>, 'CS_WoE': <tf.Tensor 'inputs_43:0' shape=(None,) dtype=float32>, 'FL_WoE': <tf.Tensor 'inputs_44:0' shape=(None,) dtype=float32>, 'GL_WoE': <tf.Tensor 'inputs_45:0' shape=(None,) dtype=float32>, 'CB_WoE': <tf.Tensor 'inputs_46:0' shape=(None,) dtype=float32>, 'DF_WoE': <tf.Tensor 'inputs_47:0' shape=(None,) dtype=float32>, 'BP_WoE': <tf.Tensor 'inputs_48:0' shape=(None,) dtype=float32>, 'EB_WoE': <tf.Tensor 'inputs_49:0' shape=(None,) dtype=float32>, 'AR_WoE': <tf.Tensor 'inputs_50:0' shape=(None,) dtype=float32>, 'CD_WoE': <tf.Tensor 'inputs_51:0' shape=(None,) dtype=float32>, 'EL_WoE': <tf.Tensor 'inputs_52:0' shape=(None,) dtype=float32>, 'CR_WoE': <tf.Tensor 'inputs_53:0' shape=(None,) dtype=float32>, 'BQ_WoE': <tf.Tensor 'inputs_54:0' shape=(None,) dtype=float32>, 'DU_na': <tf.Tensor 'inputs_55:0' shape=(None,) dtype=int64>, 'FC_na': <tf.Tensor 'inputs_56:0' shape=(None,) dtype=int64>, 'FS_na': <tf.Tensor 'inputs_57:0' shape=(None,) dtype=int64>, 'CC_na': <tf.Tensor 'inputs_58:0' shape=(None,) dtype=int64>, 'FL_na': <tf.Tensor 'inputs_59:0' shape=(None,) dtype=int64>, 'GL_na': <tf.Tensor 'inputs_60:0' shape=(None,) dtype=int64>, 'CB_na': <tf.Tensor 'inputs_61:0' shape=(None,) dtype=int64>, 'EL_na': <tf.Tensor 'inputs_62:0' shape=(None,) dtype=int64>, 'BQ_na': <tf.Tensor 'inputs_63:0' shape=(None,) dtype=int64>, 'DU': <tf.Tensor 'inputs_64:0' shape=(None,) dtype=float32>, 'BC': <tf.Tensor 'inputs_65:0' shape=(None,) dtype=float32>, 'AF': <tf.Tensor 'inputs_66:0' shape=(None,) dtype=float32>, 'EH': <tf.Tensor 'inputs_67:0' shape=(None,) dtype=float32>, 'AM': <tf.Tensor 'inputs_68:0' shape=(None,) dtype=float32>, 'FD': <tf.Tensor 'inputs_69:0' shape=(None,) dtype=float32>, 'DI': <tf.Tensor 'inputs_70:0' shape=(None,) dtype=float32>, 'FR': <tf.Tensor 'inputs_71:0' shape=(None,) dtype=float32>, 'AB': <tf.Tensor 'inputs_72:0' shape=(None,) dtype=float32>, 'CF': <tf.Tensor 'inputs_73:0' shape=(None,) dtype=float32>, 'BZ': <tf.Tensor 'inputs_74:0' shape=(None,) dtype=float32>, 'FE': <tf.Tensor 'inputs_75:0' shape=(None,) dtype=float32>, 'AX': <tf.Tensor 'inputs_76:0' shape=(None,) dtype=float32>, 'BR': <tf.Tensor 'inputs_77:0' shape=(None,) dtype=float32>, 'GB': <tf.Tensor 'inputs_78:0' shape=(None,) dtype=float32>, 'AY': <tf.Tensor 'inputs_79:0' shape=(None,) dtype=float32>, 'FC': <tf.Tensor 'inputs_80:0' shape=(None,) dtype=float32>, 'FS': <tf.Tensor 'inputs_81:0' shape=(None,) dtype=float32>, 'CC': <tf.Tensor 'inputs_82:0' shape=(None,) dtype=float32>, 'GH': <tf.Tensor 'inputs_83:0' shape=(None,) dtype=float32>, 'DV': <tf.Tensor 'inputs_84:0' shape=(None,) dtype=float32>, 'DY': <tf.Tensor 'inputs_85:0' shape=(None,) dtype=float32>, 'EE': <tf.Tensor 'inputs_86:0' shape=(None,) dtype=float32>, 'GE': <tf.Tensor 'inputs_87:0' shape=(None,) dtype=float32>, 'AH': <tf.Tensor 'inputs_88:0' shape=(None,) dtype=float32>, 'DN': <tf.Tensor 'inputs_89:0' shape=(None,) dtype=float32>, 'EP': <tf.Tensor 'inputs_90:0' shape=(None,) dtype=float32>, 'EU': <tf.Tensor 'inputs_91:0' shape=(None,) dtype=float32>, 'EG': <tf.Tensor 'inputs_92:0' shape=(None,) dtype=float32>, 'FI': <tf.Tensor 'inputs_93:0' shape=(None,) dtype=float32>, 'DH': <tf.Tensor 'inputs_94:0' shape=(None,) dtype=float32>, 'GF': <tf.Tensor 'inputs_95:0' shape=(None,) dtype=float32>, 'GI': <tf.Tensor 'inputs_96:0' shape=(None,) dtype=float32>, 'DE': <tf.Tensor 'inputs_97:0' shape=(None,) dtype=float32>, 'DA': <tf.Tensor 'inputs_98:0' shape=(None,) dtype=float32>, 'CW': <tf.Tensor 'inputs_99:0' shape=(None,) dtype=float32>, 'CU': <tf.Tensor 'inputs_100:0' shape=(None,) dtype=float32>, 'AZ': <tf.Tensor 'inputs_101:0' shape=(None,) dtype=float32>, 'CL': <tf.Tensor 'inputs_102:0' shape=(None,) dtype=float32>, 'CH': <tf.Tensor 'inputs_103:0' shape=(None,) dtype=float32>, 'BD': <tf.Tensor 'inputs_104:0' shape=(None,) dtype=float32>, 'BN': <tf.Tensor 'inputs_105:0' shape=(None,) dtype=float32>, 'DL': <tf.Tensor 'inputs_106:0' shape=(None,) dtype=float32>, 'CS': <tf.Tensor 'inputs_107:0' shape=(None,) dtype=float32>, 'FL': <tf.Tensor 'inputs_108:0' shape=(None,) dtype=float32>, 'GL': <tf.Tensor 'inputs_109:0' shape=(None,) dtype=float32>, 'CB': <tf.Tensor 'inputs_110:0' shape=(None,) dtype=float32>, 'DF': <tf.Tensor 'inputs_111:0' shape=(None,) dtype=float32>, 'BP': <tf.Tensor 'inputs_112:0' shape=(None,) dtype=float32>, 'EB': <tf.Tensor 'inputs_113:0' shape=(None,) dtype=float32>, 'AR': <tf.Tensor 'inputs_114:0' shape=(None,) dtype=float32>, 'CD': <tf.Tensor 'inputs_115:0' shape=(None,) dtype=float32>, 'EL': <tf.Tensor 'inputs_116:0' shape=(None,) dtype=float32>, 'CR': <tf.Tensor 'inputs_117:0' shape=(None,) dtype=float32>, 'BQ': <tf.Tensor 'inputs_118:0' shape=(None,) dtype=float32>, 'EJ_A': <tf.Tensor 'inputs_119:0' shape=(None,) dtype=int64>}\n1/1 [==============================] - 0s 231ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's try to find Cutoffs organically\ntrain_summary_rf_1_mean = train_summary_rf_1.mean(axis=1)\ntrain_summary_rf_1_mean.name = 'Pred'\n\ntmp = pd.concat([train_summary_rf_1_mean, train_out['Class']], axis=1).sort_index(ascending=True)\n\ntmp['Pred_Bins'] = pd.qcut(x=tmp['Pred'],q=100)\ntmp = tmp.groupby('Pred_Bins')['Class'].agg({'sum','count'})\ntmp['count_cumsum'] = tmp['count'].cumsum()\ntmp['sum_cumsum'] = tmp['sum'].cumsum()\n\ntmp['bads_rate'] = tmp['sum_cumsum']/tmp['count_cumsum']\n\ntmp['perc_sum'] = [*range(1,101)]\n\ntmp\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:39:53.780614Z","iopub.execute_input":"2023-07-28T12:39:53.781066Z","iopub.status.idle":"2023-07-28T12:39:53.833208Z","shell.execute_reply.started":"2023-07-28T12:39:53.781031Z","shell.execute_reply":"2023-07-28T12:39:53.832045Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"                                  count  sum  count_cumsum  sum_cumsum  \\\nPred_Bins                                                                \n(0.0006000000000000001, 0.00706]      7    0             7           0   \n(0.00706, 0.00979]                    6    0            13           0   \n(0.00979, 0.0128]                     6    0            19           0   \n(0.0128, 0.0153]                      6    0            25           0   \n(0.0153, 0.0175]                      6    0            31           0   \n(0.0175, 0.0186]                      6    0            37           0   \n(0.0186, 0.0206]                      7    0            44           0   \n(0.0206, 0.0236]                      6    0            50           0   \n(0.0236, 0.0259]                      6    0            56           0   \n(0.0259, 0.0282]                      6    0            62           0   \n(0.0282, 0.0301]                      6    0            68           0   \n(0.0301, 0.0312]                      6    0            74           0   \n(0.0312, 0.0322]                      7    0            81           0   \n(0.0322, 0.0337]                      6    0            87           0   \n(0.0337, 0.0348]                      6    0            93           0   \n(0.0348, 0.0369]                      6    0            99           0   \n(0.0369, 0.0382]                      6    0           105           0   \n(0.0382, 0.0396]                      6    0           111           0   \n(0.0396, 0.0408]                      7    0           118           0   \n(0.0408, 0.0424]                      6    0           124           0   \n(0.0424, 0.0453]                      6    0           130           0   \n(0.0453, 0.0467]                      6    0           136           0   \n(0.0467, 0.0483]                      6    0           142           0   \n(0.0483, 0.0498]                      6    0           148           0   \n(0.0498, 0.052]                       7    0           155           0   \n(0.052, 0.054]                        6    0           161           0   \n(0.054, 0.0567]                       6    0           167           0   \n(0.0567, 0.0579]                      6    0           173           0   \n(0.0579, 0.0597]                      6    0           179           0   \n(0.0597, 0.0616]                      6    0           185           0   \n(0.0616, 0.063]                       6    0           191           0   \n(0.063, 0.065]                        7    0           198           0   \n(0.065, 0.0671]                       6    0           204           0   \n(0.0671, 0.0694]                      6    0           210           0   \n(0.0694, 0.0716]                      6    0           216           0   \n(0.0716, 0.0728]                      6    0           222           0   \n(0.0728, 0.0738]                      6    0           228           0   \n(0.0738, 0.0766]                      7    0           235           0   \n(0.0766, 0.0799]                      6    0           241           0   \n(0.0799, 0.0827]                      6    0           247           0   \n(0.0827, 0.0849]                      6    0           253           0   \n(0.0849, 0.0874]                      7    0           260           0   \n(0.0874, 0.0901]                      5    0           265           0   \n(0.0901, 0.0924]                      7    0           272           0   \n(0.0924, 0.0946]                      6    0           278           0   \n(0.0946, 0.0978]                      6    0           284           0   \n(0.0978, 0.101]                       7    0           291           0   \n(0.101, 0.103]                        5    0           296           0   \n(0.103, 0.105]                        6    0           302           0   \n(0.105, 0.106]                        7    0           309           0   \n(0.106, 0.111]                        6    0           315           0   \n(0.111, 0.114]                        7    0           322           0   \n(0.114, 0.115]                        5    0           327           0   \n(0.115, 0.119]                        6    0           333           0   \n(0.119, 0.122]                        6    0           339           0   \n(0.122, 0.123]                        7    0           346           0   \n(0.123, 0.127]                        6    0           352           0   \n(0.127, 0.133]                        6    0           358           0   \n(0.133, 0.138]                        6    0           364           0   \n(0.138, 0.143]                        6    0           370           0   \n(0.143, 0.147]                        6    0           376           0   \n(0.147, 0.154]                        6    0           382           0   \n(0.154, 0.158]                        7    0           389           0   \n(0.158, 0.165]                        7    0           396           0   \n(0.165, 0.169]                        5    0           401           0   \n(0.169, 0.174]                        6    0           407           0   \n(0.174, 0.178]                        6    0           413           0   \n(0.178, 0.185]                        6    0           419           0   \n(0.185, 0.191]                        7    0           426           0   \n(0.191, 0.2]                          7    0           433           0   \n(0.2, 0.212]                          5    0           438           0   \n(0.212, 0.224]                        7    0           445           0   \n(0.224, 0.23]                         5    0           450           0   \n(0.23, 0.248]                         6    0           456           0   \n(0.248, 0.274]                        7    0           463           0   \n(0.274, 0.287]                        6    0           469           0   \n(0.287, 0.32]                         6    0           475           0   \n(0.32, 0.352]                         6    0           481           0   \n(0.352, 0.406]                        6    1           487           1   \n(0.406, 0.431]                        6    2           493           3   \n(0.431, 0.515]                        6    1           499           4   \n(0.515, 0.552]                        7    3           506           7   \n(0.552, 0.576]                        6    1           512           8   \n(0.576, 0.658]                        6    2           518          10   \n(0.658, 0.708]                        6    5           524          15   \n(0.708, 0.768]                        6    6           530          21   \n(0.768, 0.79]                         6    6           536          27   \n(0.79, 0.813]                         7    7           543          34   \n(0.813, 0.836]                        6    6           549          40   \n(0.836, 0.848]                        6    6           555          46   \n(0.848, 0.861]                        6    6           561          52   \n(0.861, 0.874]                        6    6           567          58   \n(0.874, 0.88]                         7    7           574          65   \n(0.88, 0.887]                         6    6           580          71   \n(0.887, 0.892]                        6    6           586          77   \n(0.892, 0.9]                          6    6           592          83   \n(0.9, 0.911]                          6    6           598          89   \n(0.911, 0.921]                        6    6           604          95   \n(0.921, 0.938]                        6    6           610         101   \n(0.938, 0.966]                        7    7           617         108   \n\n                                  bads_rate  perc_sum  \nPred_Bins                                              \n(0.0006000000000000001, 0.00706]   0.000000         1  \n(0.00706, 0.00979]                 0.000000         2  \n(0.00979, 0.0128]                  0.000000         3  \n(0.0128, 0.0153]                   0.000000         4  \n(0.0153, 0.0175]                   0.000000         5  \n(0.0175, 0.0186]                   0.000000         6  \n(0.0186, 0.0206]                   0.000000         7  \n(0.0206, 0.0236]                   0.000000         8  \n(0.0236, 0.0259]                   0.000000         9  \n(0.0259, 0.0282]                   0.000000        10  \n(0.0282, 0.0301]                   0.000000        11  \n(0.0301, 0.0312]                   0.000000        12  \n(0.0312, 0.0322]                   0.000000        13  \n(0.0322, 0.0337]                   0.000000        14  \n(0.0337, 0.0348]                   0.000000        15  \n(0.0348, 0.0369]                   0.000000        16  \n(0.0369, 0.0382]                   0.000000        17  \n(0.0382, 0.0396]                   0.000000        18  \n(0.0396, 0.0408]                   0.000000        19  \n(0.0408, 0.0424]                   0.000000        20  \n(0.0424, 0.0453]                   0.000000        21  \n(0.0453, 0.0467]                   0.000000        22  \n(0.0467, 0.0483]                   0.000000        23  \n(0.0483, 0.0498]                   0.000000        24  \n(0.0498, 0.052]                    0.000000        25  \n(0.052, 0.054]                     0.000000        26  \n(0.054, 0.0567]                    0.000000        27  \n(0.0567, 0.0579]                   0.000000        28  \n(0.0579, 0.0597]                   0.000000        29  \n(0.0597, 0.0616]                   0.000000        30  \n(0.0616, 0.063]                    0.000000        31  \n(0.063, 0.065]                     0.000000        32  \n(0.065, 0.0671]                    0.000000        33  \n(0.0671, 0.0694]                   0.000000        34  \n(0.0694, 0.0716]                   0.000000        35  \n(0.0716, 0.0728]                   0.000000        36  \n(0.0728, 0.0738]                   0.000000        37  \n(0.0738, 0.0766]                   0.000000        38  \n(0.0766, 0.0799]                   0.000000        39  \n(0.0799, 0.0827]                   0.000000        40  \n(0.0827, 0.0849]                   0.000000        41  \n(0.0849, 0.0874]                   0.000000        42  \n(0.0874, 0.0901]                   0.000000        43  \n(0.0901, 0.0924]                   0.000000        44  \n(0.0924, 0.0946]                   0.000000        45  \n(0.0946, 0.0978]                   0.000000        46  \n(0.0978, 0.101]                    0.000000        47  \n(0.101, 0.103]                     0.000000        48  \n(0.103, 0.105]                     0.000000        49  \n(0.105, 0.106]                     0.000000        50  \n(0.106, 0.111]                     0.000000        51  \n(0.111, 0.114]                     0.000000        52  \n(0.114, 0.115]                     0.000000        53  \n(0.115, 0.119]                     0.000000        54  \n(0.119, 0.122]                     0.000000        55  \n(0.122, 0.123]                     0.000000        56  \n(0.123, 0.127]                     0.000000        57  \n(0.127, 0.133]                     0.000000        58  \n(0.133, 0.138]                     0.000000        59  \n(0.138, 0.143]                     0.000000        60  \n(0.143, 0.147]                     0.000000        61  \n(0.147, 0.154]                     0.000000        62  \n(0.154, 0.158]                     0.000000        63  \n(0.158, 0.165]                     0.000000        64  \n(0.165, 0.169]                     0.000000        65  \n(0.169, 0.174]                     0.000000        66  \n(0.174, 0.178]                     0.000000        67  \n(0.178, 0.185]                     0.000000        68  \n(0.185, 0.191]                     0.000000        69  \n(0.191, 0.2]                       0.000000        70  \n(0.2, 0.212]                       0.000000        71  \n(0.212, 0.224]                     0.000000        72  \n(0.224, 0.23]                      0.000000        73  \n(0.23, 0.248]                      0.000000        74  \n(0.248, 0.274]                     0.000000        75  \n(0.274, 0.287]                     0.000000        76  \n(0.287, 0.32]                      0.000000        77  \n(0.32, 0.352]                      0.000000        78  \n(0.352, 0.406]                     0.002053        79  \n(0.406, 0.431]                     0.006085        80  \n(0.431, 0.515]                     0.008016        81  \n(0.515, 0.552]                     0.013834        82  \n(0.552, 0.576]                     0.015625        83  \n(0.576, 0.658]                     0.019305        84  \n(0.658, 0.708]                     0.028626        85  \n(0.708, 0.768]                     0.039623        86  \n(0.768, 0.79]                      0.050373        87  \n(0.79, 0.813]                      0.062615        88  \n(0.813, 0.836]                     0.072860        89  \n(0.836, 0.848]                     0.082883        90  \n(0.848, 0.861]                     0.092692        91  \n(0.861, 0.874]                     0.102293        92  \n(0.874, 0.88]                      0.113240        93  \n(0.88, 0.887]                      0.122414        94  \n(0.887, 0.892]                     0.131399        95  \n(0.892, 0.9]                       0.140203        96  \n(0.9, 0.911]                       0.148829        97  \n(0.911, 0.921]                     0.157285        98  \n(0.921, 0.938]                     0.165574        99  \n(0.938, 0.966]                     0.175041       100  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>sum</th>\n      <th>count_cumsum</th>\n      <th>sum_cumsum</th>\n      <th>bads_rate</th>\n      <th>perc_sum</th>\n    </tr>\n    <tr>\n      <th>Pred_Bins</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(0.0006000000000000001, 0.00706]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>(0.00706, 0.00979]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>(0.00979, 0.0128]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>19</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>(0.0128, 0.0153]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>(0.0153, 0.0175]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>31</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>(0.0175, 0.0186]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>(0.0186, 0.0206]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>(0.0206, 0.0236]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>50</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>(0.0236, 0.0259]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>56</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>(0.0259, 0.0282]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>(0.0282, 0.0301]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>68</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>(0.0301, 0.0312]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>(0.0312, 0.0322]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>81</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>(0.0322, 0.0337]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>87</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>(0.0337, 0.0348]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>93</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>(0.0348, 0.0369]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>99</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>(0.0369, 0.0382]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>105</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>(0.0382, 0.0396]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>111</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>(0.0396, 0.0408]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>118</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>(0.0408, 0.0424]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>124</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>(0.0424, 0.0453]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>130</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>(0.0453, 0.0467]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>136</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>(0.0467, 0.0483]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>142</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>(0.0483, 0.0498]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>148</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>(0.0498, 0.052]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>155</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>(0.052, 0.054]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>161</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>(0.054, 0.0567]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>167</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>(0.0567, 0.0579]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>173</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>(0.0579, 0.0597]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>179</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>(0.0597, 0.0616]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>185</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>(0.0616, 0.063]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>191</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>(0.063, 0.065]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>198</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>(0.065, 0.0671]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>204</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>(0.0671, 0.0694]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>210</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>(0.0694, 0.0716]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>216</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>(0.0716, 0.0728]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>222</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>(0.0728, 0.0738]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>228</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>(0.0738, 0.0766]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>235</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>(0.0766, 0.0799]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>241</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>(0.0799, 0.0827]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>247</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>(0.0827, 0.0849]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>253</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>(0.0849, 0.0874]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>260</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>(0.0874, 0.0901]</th>\n      <td>5</td>\n      <td>0</td>\n      <td>265</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>(0.0901, 0.0924]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>(0.0924, 0.0946]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>278</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>(0.0946, 0.0978]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>284</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>(0.0978, 0.101]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>291</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>(0.101, 0.103]</th>\n      <td>5</td>\n      <td>0</td>\n      <td>296</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>(0.103, 0.105]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>302</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>(0.105, 0.106]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>309</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>(0.106, 0.111]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>315</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>(0.111, 0.114]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>322</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>(0.114, 0.115]</th>\n      <td>5</td>\n      <td>0</td>\n      <td>327</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>(0.115, 0.119]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>333</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>(0.119, 0.122]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>339</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>(0.122, 0.123]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>346</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>(0.123, 0.127]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>352</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>(0.127, 0.133]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>358</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>(0.133, 0.138]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>364</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>(0.138, 0.143]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>370</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>(0.143, 0.147]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>376</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>(0.147, 0.154]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>382</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>(0.154, 0.158]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>389</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>(0.158, 0.165]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>396</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>(0.165, 0.169]</th>\n      <td>5</td>\n      <td>0</td>\n      <td>401</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>(0.169, 0.174]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>407</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>(0.174, 0.178]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>413</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>(0.178, 0.185]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>419</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>(0.185, 0.191]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>426</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>(0.191, 0.2]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>433</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>(0.2, 0.212]</th>\n      <td>5</td>\n      <td>0</td>\n      <td>438</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>(0.212, 0.224]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>445</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>(0.224, 0.23]</th>\n      <td>5</td>\n      <td>0</td>\n      <td>450</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>(0.23, 0.248]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>456</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>(0.248, 0.274]</th>\n      <td>7</td>\n      <td>0</td>\n      <td>463</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>(0.274, 0.287]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>469</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>(0.287, 0.32]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>475</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>(0.32, 0.352]</th>\n      <td>6</td>\n      <td>0</td>\n      <td>481</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>(0.352, 0.406]</th>\n      <td>6</td>\n      <td>1</td>\n      <td>487</td>\n      <td>1</td>\n      <td>0.002053</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>(0.406, 0.431]</th>\n      <td>6</td>\n      <td>2</td>\n      <td>493</td>\n      <td>3</td>\n      <td>0.006085</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>(0.431, 0.515]</th>\n      <td>6</td>\n      <td>1</td>\n      <td>499</td>\n      <td>4</td>\n      <td>0.008016</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>(0.515, 0.552]</th>\n      <td>7</td>\n      <td>3</td>\n      <td>506</td>\n      <td>7</td>\n      <td>0.013834</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>(0.552, 0.576]</th>\n      <td>6</td>\n      <td>1</td>\n      <td>512</td>\n      <td>8</td>\n      <td>0.015625</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>(0.576, 0.658]</th>\n      <td>6</td>\n      <td>2</td>\n      <td>518</td>\n      <td>10</td>\n      <td>0.019305</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>(0.658, 0.708]</th>\n      <td>6</td>\n      <td>5</td>\n      <td>524</td>\n      <td>15</td>\n      <td>0.028626</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>(0.708, 0.768]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>530</td>\n      <td>21</td>\n      <td>0.039623</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>(0.768, 0.79]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>536</td>\n      <td>27</td>\n      <td>0.050373</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>(0.79, 0.813]</th>\n      <td>7</td>\n      <td>7</td>\n      <td>543</td>\n      <td>34</td>\n      <td>0.062615</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>(0.813, 0.836]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>549</td>\n      <td>40</td>\n      <td>0.072860</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>(0.836, 0.848]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>555</td>\n      <td>46</td>\n      <td>0.082883</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>(0.848, 0.861]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>561</td>\n      <td>52</td>\n      <td>0.092692</td>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>(0.861, 0.874]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>567</td>\n      <td>58</td>\n      <td>0.102293</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>(0.874, 0.88]</th>\n      <td>7</td>\n      <td>7</td>\n      <td>574</td>\n      <td>65</td>\n      <td>0.113240</td>\n      <td>93</td>\n    </tr>\n    <tr>\n      <th>(0.88, 0.887]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>580</td>\n      <td>71</td>\n      <td>0.122414</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>(0.887, 0.892]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>586</td>\n      <td>77</td>\n      <td>0.131399</td>\n      <td>95</td>\n    </tr>\n    <tr>\n      <th>(0.892, 0.9]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>592</td>\n      <td>83</td>\n      <td>0.140203</td>\n      <td>96</td>\n    </tr>\n    <tr>\n      <th>(0.9, 0.911]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>598</td>\n      <td>89</td>\n      <td>0.148829</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>(0.911, 0.921]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>604</td>\n      <td>95</td>\n      <td>0.157285</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>(0.921, 0.938]</th>\n      <td>6</td>\n      <td>6</td>\n      <td>610</td>\n      <td>101</td>\n      <td>0.165574</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>(0.938, 0.966]</th>\n      <td>7</td>\n      <td>7</td>\n      <td>617</td>\n      <td>108</td>\n      <td>0.175041</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# RandomForestModel","metadata":{}},{"cell_type":"code","source":"# RandomForestModel\nif not SUBMITION:\n    train_summary_rf_1, valid_summary_rf_1, test_summary_rf_1, model_rf_1,metrics_rf_1 = train_model(\n        train=train_out,test=test_out, features=features,\n        n_splits=6,\n        model_obj=tfdf.keras.RandomForestModel,\n        model_kwargs=dict(max_depth=6, num_trees=1000),\n        model_compile_kwargs=dict(metrics=[metrics.binary_accuracy,BalancedLogLoss()]))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:28:58.714677Z","iopub.execute_input":"2023-07-22T06:28:58.715036Z","iopub.status.idle":"2023-07-22T06:29:54.982233Z","shell.execute_reply.started":"2023-07-22T06:28:58.714999Z","shell.execute_reply":"2023-07-22T06:29:54.980987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RandomForestModel\nif not SUBMITION:\n    train_summary_rf_2, valid_summary_rf_2, test_summary_rf_2, model_rf_2,metrics_rf_2 = train_model(\n        train=train_out,test=test_out, features=features,                                             \n        n_splits=10,                    \n        model_obj=tfdf.keras.RandomForestModel,           \n        model_kwargs=dict(max_depth=6, num_trees=300),                                                  \n        model_compile_kwargs=dict(metrics=[metrics.binary_accuracy,BalancedLogLoss()]))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:29:54.983882Z","iopub.execute_input":"2023-07-22T06:29:54.984307Z","iopub.status.idle":"2023-07-22T06:31:10.290925Z","shell.execute_reply.started":"2023-07-22T06:29:54.984274Z","shell.execute_reply":"2023-07-22T06:31:10.289651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RandomForestModel\n\nfeatures_slice = [i for i in features if len(i) > 2]\n\ntrain_summary_rf_3, valid_summary_rf_3, test_summary_rf_3, model_rf_3,metrics_rf_3 = train_model(\n    train=train_out,test=test_out, features=features_slice,                                             \n    n_splits=10,                    \n    model_obj=tfdf.keras.RandomForestModel,           \n    model_kwargs=dict(max_depth=6, num_trees=300),                                                  \n    model_compile_kwargs=dict(metrics=[metrics.binary_accuracy,BalancedLogLoss()]))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:31:10.292950Z","iopub.execute_input":"2023-07-22T06:31:10.293438Z","iopub.status.idle":"2023-07-22T06:31:56.937132Z","shell.execute_reply.started":"2023-07-22T06:31:10.293396Z","shell.execute_reply":"2023-07-22T06:31:56.936084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RandomForestModel\n\nfeatures_slice = [i for i in features if len(i) > 2]\n\ntrain_summary_rf_3, valid_summary_rf_3, test_summary_rf_3, model_rf_3,metrics_rf_3 = train_model(\n    train=train_out,test=test_out, features=features_slice,                                             \n    n_splits=10,                    \n    model_obj=tfdf.keras.RandomForestModel,           \n    model_kwargs=dict(max_depth=6, num_trees=300),                                                  \n    model_compile_kwargs=dict(metrics=[metrics.binary_accuracy,BalancedLogLoss()]))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:31:56.938508Z","iopub.execute_input":"2023-07-22T06:31:56.940343Z","iopub.status.idle":"2023-07-22T06:32:43.537911Z","shell.execute_reply.started":"2023-07-22T06:31:56.940307Z","shell.execute_reply":"2023-07-22T06:32:43.536602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GradientBoostedTreesModel","metadata":{}},{"cell_type":"code","source":"# GradientBoostedTreesModel\n\nif not SUBMITION:\n\n    features_slice = [i for i in features if len(i) > 2]\n\n    train_summary_gbt_1, valid_summary_gbt_1, test_summary_gbt_1, model_gbt_1,metrics_gbt_1 = train_model(\n        train=train_out,test=test_out, features=features_slice,                                             \n        n_splits=10,                    \n        model_obj=tfdf.keras.GradientBoostedTreesModel,            \n        model_kwargs=dict(max_depth=5, num_trees=1000),                                                  \n        model_compile_kwargs=dict(metrics=[metrics.binary_accuracy, BalancedLogLoss()]))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:32:43.540450Z","iopub.execute_input":"2023-07-22T06:32:43.541019Z","iopub.status.idle":"2023-07-22T06:33:49.767994Z","shell.execute_reply.started":"2023-07-22T06:32:43.540985Z","shell.execute_reply":"2023-07-22T06:33:49.765943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GradientBoostedTreesModel\n\nfeatures_slice = [i for i in features if len(i) > 2]\n\ntrain_summary_gbt_2, valid_summary_gbt_2, test_summary_gbt_2, model_gbt_2,metrics_gbt_2 = train_model(\n    train=train_out,test=test_out, features=features_slice,                                             \n    n_splits=10,                    \n    model_obj=tfdf.keras.GradientBoostedTreesModel,            \n    model_kwargs=dict(max_depth=2, num_trees=1000),                                                  \n    model_compile_kwargs=dict(metrics=[metrics.binary_accuracy, BalancedLogLoss()]))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:33:49.769795Z","iopub.execute_input":"2023-07-22T06:33:49.771485Z","iopub.status.idle":"2023-07-22T06:34:46.177812Z","shell.execute_reply.started":"2023-07-22T06:33:49.771433Z","shell.execute_reply":"2023-07-22T06:34:46.176573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TabPFN","metadata":{}},{"cell_type":"code","source":"def balanced_logloss_np(y_true: np.array, y_pred: np.array) -> float:\n    \n    # Correct Values\n    min_val = 1e-15\n    max_val = 0.999999999999999\n\n    y_pred = np.minimum(y_pred, [max_val])\n    y_pred = np.maximum(y_pred, [min_val])\n    \n    y_pred_1 = y_pred\n    y_pred_0 = 1-y_pred\n\n    log_y_pred_1 = np.reshape(np.log(y_pred_1),[-1,1])\n    log_y_pred_0 = np.reshape(np.log(y_pred_0),[-1,1])\n\n    y_1 = np.reshape(y_true,[1,-1])\n    y_0 = (y_1-1)*(-1)\n\n    logloss_1 = -np.dot(y_1,log_y_pred_1)[0][0]/np.sum(y_1)\n    logloss_0 = -np.dot(y_0,log_y_pred_0)[0][0]/np.sum(y_0)\n\n    av_logloss = (logloss_1+logloss_0)/2\n    \n    return av_logloss\n\ndef train_model_tabpfn_one(train: pd.DataFrame, test: pd.DataFrame, features: list, label = \"Class\") -> (pd.DataFrame, dict, dict):\n\n    # Create a dataframe of required size with zero values.\n    test_summary = pd.DataFrame(data=np.full((len(test.index),1), np.nan),index=test.index)\n    train_summary = pd.DataFrame(data=np.full((len(train.index),1), np.nan),index=train.index)\n    # Create an empty dictionary to store the models trained for each fold.\n    metrics = {}\n\n    # Select only feature columns for training.\n    train_df = train[features+[label]]\n\n    # Define & Train the model and metrics\n    model = TabPFNClassifier(N_ensemble_configurations=64)\n    model.fit(train_df[features],train_df[label])\n\n    # Make predictions\n    p_train = model.predict_proba(train_df[features])[:,1]\n    p_test = model.predict_proba(test[features])[:,1]\n\n    # Predict value for validation/Submition data\n    test_summary[0] = p_test.flatten() \n    train_summary[0] = p_train.flatten() \n    \n    # Evaluate and store the metrics in respective dicts\n    metrics['balanced_logloss'] = balanced_logloss_np(y_true=train_df[label].values,y_pred=p_train)\n    print(f\"\\nTrain: {metrics['balanced_logloss']:.4f}\")\n    \n            \n    return train_summary, test_summary, model, metrics\n\n\ndef train_model_tabpfn_cv(train: pd.DataFrame, test: pd.DataFrame, features: list, label = \"Class\",\n                n_splits: int = 6) -> (pd.DataFrame, dict,  dict):\n\n    # Create a various frames\n    train_summary = pd.DataFrame(data=np.full((len(train.index),n_splits), np.nan), index=train.index) # For In-Sample Predictions of each Fold\n    \n    valid_summary = pd.DataFrame(data=np.full((len(train.index),1), np.nan), index=train.index) # For Out-of-Sample Prediction of each Fold\n    \n    test_summary = pd.DataFrame(data=np.full((len(test.index),n_splits), np.nan),index=test.index) # For Test (Sumbition) Predictions of each Fold's Model\n    \n\n    # Create an empty dictionary to store the models trained for each fold.\n    models = {}\n    metrics = {}\n    balanced_logloss_train = {}\n    balanced_logloss_val = {}\n    \n    # Loop through each fold\n    skf = StratifiedKFold(n_splits=n_splits)\n    \n    for i, (train_index, valid_index) in enumerate(skf.split(X=train,y=train['Class'])):\n            print('##### Fold',i+1)\n\n            # Fetch values corresponding to the index \n            train_df = train.iloc[train_index]\n            valid_df = train.iloc[valid_index]\n            train_ids = train_df.index.values\n            valid_ids = valid_df.index.values\n\n            # Select only feature columns for training.\n            train_df = train_df[features+[label]]\n            valid_df = valid_df[features+[label]]\n\n            # Define & Train the model\n            model = TabPFNClassifier(N_ensemble_configurations=64)\n            model.fit(train_df[features],train_df[label])\n\n            # Store the model\n            models[f\"fold_{i+1}\"] = model\n\n            # Predict value for validation/Submition data\n            p_train = model.predict_proba(train_df[features])[:,1]\n            p_val = model.predict_proba(valid_df[features])[:,1]\n            p_sub = model.predict_proba(test[features])[:,1]\n            \n            # Predict Values\n            train_summary.loc[train_ids, i] = p_train\n            valid_summary.loc[valid_ids, 0] = p_val\n            test_summary[i] = p_sub\n\n            # Evaluate and store the metrics in respective dicts\n            train_metric = balanced_logloss_np(y_true=train_df[label].values,y_pred=p_train)\n            val_metric = balanced_logloss_np(y_true=valid_df[label].values,y_pred=p_val)\n            \n            balanced_logloss_train[f\"fold_{i+1}\"] = train_metric\n            balanced_logloss_val[f\"fold_{i+1}\"] = val_metric\n            \n            print(f\"\\nTrain: {train_metric:.4f} Validation: {val_metric:.4f}\")\n    \n    metrics['train'] = balanced_logloss_train\n    metrics['val'] = balanced_logloss_val\n\n    print(f\"\\nTrain mean: {pd.Series(balanced_logloss_train).mean():.4f} std: {pd.Series(balanced_logloss_train).std():.4f}\")\n    print(f\"\\nValidation mean: {pd.Series(balanced_logloss_val).mean():.4f} std: {pd.Series(balanced_logloss_val).std():.4f}\")\n            \n    return train_summary, valid_summary, test_summary, models,metrics","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:34:46.179640Z","iopub.execute_input":"2023-07-22T06:34:46.180125Z","iopub.status.idle":"2023-07-22T06:34:46.209710Z","shell.execute_reply.started":"2023-07-22T06:34:46.180093Z","shell.execute_reply":"2023-07-22T06:34:46.208418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TabPFN tabpfn_\nif not SUBMITION:\n    features_slice = [i for i in features if len(i) > 2]\n    train_summary_tabpfn_1, test_summary_tabpfn_1, model_tabpfn_1, metrics_tabpfn_1 = train_model_tabpfn_one(\n        train=train_out,test=test_out, features=features_slice)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:36:27.489677Z","iopub.execute_input":"2023-07-22T07:36:27.490161Z","iopub.status.idle":"2023-07-22T07:38:28.930962Z","shell.execute_reply.started":"2023-07-22T07:36:27.490111Z","shell.execute_reply":"2023-07-22T07:38:28.929257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TabPFN tabpfn_\nfeatures_slice = [i for i in features if len(i) > 2]\ntrain_summary_tabpfn_2, valid_summary_tabpfn_2, test_summary_tabpfn_2, model_tabpfn_2, metrics_tabpfn_2 = train_model_tabpfn_cv(\n    train=train_out,test=test_out, features=features_slice, n_splits=6)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T06:36:40.207653Z","iopub.execute_input":"2023-07-22T06:36:40.208129Z","iopub.status.idle":"2023-07-22T06:49:24.120496Z","shell.execute_reply.started":"2023-07-22T06:36:40.208021Z","shell.execute_reply":"2023-07-22T06:49:24.119160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble CV","metadata":{}},{"cell_type":"code","source":"class EnsembleCV(object):\n    def __init__(self):\n        self.is_fitted = False\n        self.weights = None\n        return\n    \n    def fit(self, X: pd.DataFrame) -> None:\n        self.weights = None\n        return\n    \n    @staticmethod\n    def predict(X: pd.DataFrame) -> pd.Series:\n        # Take conservative estimate\n        X_ = X.mean(axis=1) #.to_frame()\n        # Fillna by 1\n        X_ = X_.fillna(1)\n        return X_","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:34:05.234849Z","iopub.execute_input":"2023-07-22T07:34:05.235468Z","iopub.status.idle":"2023-07-22T07:34:05.245644Z","shell.execute_reply.started":"2023-07-22T07:34:05.235422Z","shell.execute_reply":"2023-07-22T07:34:05.244405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_ensemble = EnsembleCV()\ngbt_ensemble = EnsembleCV()\ntabpfn_ensemble = EnsembleCV()\n\ntest_rf_ensemble = rf_ensemble.predict(X=test_summary_rf_3)\ntest_gbt_ensemble = gbt_ensemble.predict(X=test_summary_gbt_2)\ntest_tabpfn_ensemble = tabpfn_ensemble.predict(X=test_summary_tabpfn_1)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:52:38.564875Z","iopub.execute_input":"2023-07-22T07:52:38.565353Z","iopub.status.idle":"2023-07-22T07:52:38.574034Z","shell.execute_reply.started":"2023-07-22T07:52:38.565320Z","shell.execute_reply":"2023-07-22T07:52:38.573051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_out['Class_RF'] = rf_ensemble.predict(X=train_summary_rf_3)\ntest_out['Class_RF'] = rf_ensemble.predict(X=test_summary_rf_3)\n\nprint(pd.Series(metrics_rf_3['train']).mean())\nprint(pd.Series(metrics_rf_3['val']).mean())\nprint(balanced_logloss_np(y_true=train_out['Class'].values,y_pred=train_out['Class_RF'].values))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:52:39.674849Z","iopub.execute_input":"2023-07-22T07:52:39.675348Z","iopub.status.idle":"2023-07-22T07:52:39.689348Z","shell.execute_reply.started":"2023-07-22T07:52:39.675309Z","shell.execute_reply":"2023-07-22T07:52:39.687879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_out['Class_GBT'] = gbt_ensemble.predict(X=train_summary_gbt_2)\ntest_out['Class_GBT'] = gbt_ensemble.predict(X=test_summary_gbt_2)\n\nprint(pd.Series(metrics_gbt_2['train']).mean())\nprint(pd.Series(metrics_gbt_2['val']).mean())\nprint(balanced_logloss_np(y_true=train_out['Class'].values,y_pred=train_out['Class_GBT'].values))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:52:40.501860Z","iopub.execute_input":"2023-07-22T07:52:40.502785Z","iopub.status.idle":"2023-07-22T07:52:40.516654Z","shell.execute_reply.started":"2023-07-22T07:52:40.502747Z","shell.execute_reply":"2023-07-22T07:52:40.515227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_out['Class_TABPFN'] = tabpfn_ensemble.predict(X=train_summary_tabpfn_1)\ntest_out['Class_TABPFN'] = tabpfn_ensemble.predict(X=test_summary_tabpfn_1)\n\nprint(pd.Series(metrics_tabpfn_2['train']).mean())\nprint(pd.Series(metrics_tabpfn_2['val']).mean())\nprint(balanced_logloss_np(y_true=train_out['Class'].values,y_pred=train_out['Class_TABPFN'].values))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:53:01.580168Z","iopub.execute_input":"2023-07-22T07:53:01.580711Z","iopub.status.idle":"2023-07-22T07:53:01.594860Z","shell.execute_reply.started":"2023-07-22T07:53:01.580672Z","shell.execute_reply":"2023-07-22T07:53:01.593705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Classes","metadata":{}},{"cell_type":"markdown","source":"I can for example use GBT","metadata":{}},{"cell_type":"code","source":"train_out[['Class_RF','Class_GBT','Class_TABPFN','Class']].corr()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:53:04.251819Z","iopub.execute_input":"2023-07-22T07:53:04.252606Z","iopub.status.idle":"2023-07-22T07:53:04.268921Z","shell.execute_reply.started":"2023-07-22T07:53:04.252566Z","shell.execute_reply":"2023-07-22T07:53:04.267406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GradientBoostedTreesModel\n# Got Very Bas score up to 1.5 in LB\n# train_summary_gbt_3, valid_summary_gbt_3, test_summary_gbt_3, model_gbt_3,metrics_gbt_3 = train_model(\n#     train=train_out,test=test_out, features=['Class_RF','Class_GBT','Class_TABPFN'],                                             \n#     n_splits=10,                    \n#     model_obj=tfdf.keras.GradientBoostedTreesModel,            \n#     model_kwargs=dict(max_depth=2, num_trees=1000),                                                  \n#     model_compile_kwargs=dict(metrics=[metrics.binary_accuracy, BalancedLogLoss()]))","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:49:11.787044Z","iopub.execute_input":"2023-07-22T07:49:11.787580Z","iopub.status.idle":"2023-07-22T07:49:11.793938Z","shell.execute_reply.started":"2023-07-22T07:49:11.787542Z","shell.execute_reply":"2023-07-22T07:49:11.792500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF_w = 2.5/10\nGBT_w = 4.5/10\nTabPFN_w = 3./10\n\nprint('RF weight: ', RF_w)\nprint('GBT weight: ', GBT_w)\nprint('TabPFN weight: ', TabPFN_w)\n\nsubmition_total = test_out[['Class_RF','Class_GBT','Class_TABPFN']].copy()\nsubmition_total.columns = ['RF','GBT','TabPFN']\n\nsubmition_total['Ensemble'] = RF_w*submition_total['RF'] + GBT_w*submition_total['GBT'] + TabPFN_w*submition_total['TabPFN']\n\nsubmition_total.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:57:39.915443Z","iopub.execute_input":"2023-07-22T07:57:39.915856Z","iopub.status.idle":"2023-07-22T07:57:39.936041Z","shell.execute_reply.started":"2023-07-22T07:57:39.915827Z","shell.execute_reply":"2023-07-22T07:57:39.934959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# submition_total\nsubmition_total['Ensemble'] = np.where(submition_total['Ensemble']>0.95,1,submition_total['Ensemble'])\nsubmition_total['Ensemble'] = np.where(submition_total['Ensemble']<0.05,0,submition_total['Ensemble'])\nsubmition_total['class_1'] = submition_total['Ensemble']\nsubmition_total['class_0'] = 1 - submition_total['class_1']\nsubmition_total = submition_total[['class_0','class_1']].copy()\nsubmition_total.to_csv('/kaggle/working/submission.csv', index=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:57:42.904140Z","iopub.execute_input":"2023-07-22T07:57:42.905178Z","iopub.status.idle":"2023-07-22T07:57:42.917128Z","shell.execute_reply.started":"2023-07-22T07:57:42.905108Z","shell.execute_reply":"2023-07-22T07:57:42.916023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submition_total.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:57:47.498228Z","iopub.execute_input":"2023-07-22T07:57:47.498695Z","iopub.status.idle":"2023-07-22T07:57:47.512824Z","shell.execute_reply.started":"2023-07-22T07:57:47.498654Z","shell.execute_reply":"2023-07-22T07:57:47.511205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submition_total.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T07:57:53.687490Z","iopub.execute_input":"2023-07-22T07:57:53.687928Z","iopub.status.idle":"2023-07-22T07:57:53.704897Z","shell.execute_reply.started":"2023-07-22T07:57:53.687897Z","shell.execute_reply":"2023-07-22T07:57:53.703952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}